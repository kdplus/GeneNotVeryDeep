{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle as p\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.ops import standard_ops\n",
    "%matplotlib inline\n",
    "\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=False)\n",
    "\n",
    "def strip_first_col(fname, delimiter=None):\n",
    "    with open(fname, 'rb') as fin:\n",
    "        for line in fin:\n",
    "            try:\n",
    "               yield line.split(delimiter, 1)[1]\n",
    "            except IndexError:\n",
    "               continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.load(\"./train10_data.npy\")\n",
    "train_label = np.load(\"./train10_label.npy\")\n",
    "test_data = np.load(\"./test10_data.npy\")\n",
    "test_label = np.load(\"./test10_label.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoencoder pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l1_regularizer(scale, x):\n",
    "    return standard_ops.multiply(scale, standard_ops.reduce_sum(standard_ops.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    if convolutional:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n",
    "    else:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    update_moving_everages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "    return Ybn, update_moving_everages\n",
    "\n",
    "def no_batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n",
    "    return Ylogits, tf.no_op()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CAPACITY = 30000\n",
    "training_epochs = 10\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "n_input = 22283 #Need to change to 22283\n",
    "l1_scale = 0.001\n",
    "l2_scale = 0.001\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\",[None,n_input])\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "# X = get_batch(data,BATCH_SIZE,CAPACITY)\n",
    "tst = tf.placeholder(tf.bool)\n",
    "# training iteration\n",
    "iter = tf.placeholder(tf.int32)\n",
    "keep_prob = tf.placeholder(tf.float32)  \n",
    "\n",
    "#need to change to 4 layers \n",
    "n_hidden_1 = 200\n",
    "n_hidden_2 = 150\n",
    "n_hidden_3 = 100\n",
    "n_hidden_4 = 42\n",
    "\n",
    "weights= {\n",
    "    'encoder_h1': tf.Variable(tf.truncated_normal([n_input,n_hidden_1], stddev=0.1)),\n",
    "    'encoder_h2': tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2], stddev=0.1)),\n",
    "    'encoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_3], stddev=0.1)),\n",
    "    'encoder_h4': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_4], stddev=0.1)),\n",
    "    'decoder_h1': tf.Variable(tf.truncated_normal([n_hidden_4,n_hidden_3], stddev=0.1)),\n",
    "    'decoder_h2': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_2], stddev=0.1)),\n",
    "    'decoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_1], stddev=0.1)),\n",
    "    'decoder_h4': tf.Variable(tf.truncated_normal([n_hidden_1,n_input], stddev=0.1)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'encoder_b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'encoder_b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b3': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b4': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "# weightse1_summary = tf.summary.histogram(\"weights_e1\", weights['encoder_h1'])\n",
    "# weightse2_summary = tf.summary.histogram(\"weights_e2\", weights['encoder_h2'])\n",
    "# weightse3_summary = tf.summary.histogram(\"weights_e3\", weights['encoder_h3'])\n",
    "# weightsd1_summary = tf.summary.histogram(\"weights_d1\", weights['decoder_h1'])\n",
    "# weightsd2_summary = tf.summary.histogram(\"weights_d2\", weights['decoder_h1'])\n",
    "# weightsd3_summary = tf.summary.histogram(\"weights_d3\", weights['decoder_h1'])\n",
    "\n",
    "update_ema = []\n",
    "update_emA = []\n",
    "def encoder(x):  \n",
    "    Y1l = tf.add(tf.matmul(x, weights['encoder_h1']),biases['encoder_b1'])\n",
    "    Y1bn, update_ema1 = batchnorm(Y1l, tst, iter, biases['encoder_b1'])\n",
    "    layer_1 = tf.nn.relu(tf.nn.dropout(Y1bn, keep_prob))\n",
    "    \n",
    "    Y2l = tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2'])\n",
    "    Y2bn, update_ema2 = batchnorm(Y2l, tst, iter, biases['encoder_b2'])\n",
    "    layer_2 = tf.nn.relu(tf.nn.dropout(Y2bn, keep_prob))\n",
    "    \n",
    "    Y3l = tf.add(tf.matmul(layer_2, weights['encoder_h3']),biases['encoder_b3'])\n",
    "    Y3bn, update_ema3 = batchnorm(Y3l, tst, iter, biases['encoder_b3'])\n",
    "    layer_3 = tf.nn.relu(tf.nn.dropout(Y3bn, keep_prob))\n",
    "    \n",
    "    Y4l = tf.add(tf.matmul(layer_3, weights['encoder_h4']),biases['encoder_b4'])\n",
    "    Y4bn, update_ema4 = batchnorm(Y4l, tst, iter, biases['encoder_b4'])\n",
    "    layer_4 = tf.nn.relu(Y4bn)\n",
    "    \n",
    "    global update_ema\n",
    "    update_emA = tf.group(update_ema1, update_ema2, update_ema3, update_ema4)\n",
    "    return layer_4\n",
    "  \n",
    "def decoder(x):  \n",
    "#     layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['decoder_h1']),  biases['decoder_b1']))  \n",
    "#     layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))  \n",
    "#     layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['decoder_h3']),biases['decoder_b3']))\n",
    "#     layer_4 = tf.nn.relu(tf.add(tf.matmul(layer_3, weights['decoder_h4']),biases['decoder_b4']))\n",
    "    Y1l = tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1'])\n",
    "    Y1bn, update_ema5 = batchnorm(Y1l, tst, iter, biases['decoder_b1'])\n",
    "    layer_1 = tf.nn.relu(tf.nn.dropout(Y1bn, keep_prob))\n",
    "    \n",
    "    Y2l = tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2'])\n",
    "    Y2bn, update_ema6 = batchnorm(Y2l, tst, iter, biases['decoder_b2'])\n",
    "    layer_2 = tf.nn.relu(tf.nn.dropout(Y2bn, keep_prob))\n",
    "\n",
    "    Y3l = tf.add(tf.matmul(layer_2, weights['decoder_h3']),biases['decoder_b3'])\n",
    "    Y3bn, update_ema7 = batchnorm(Y3l, tst, iter, biases['decoder_b3'])\n",
    "    layer_3 = tf.nn.relu(tf.nn.dropout(Y3bn, keep_prob))\n",
    "    \n",
    "    Y4l = tf.add(tf.matmul(layer_3, weights['decoder_h4']),biases['decoder_b4'])\n",
    "    Y4bn, update_ema8 = batchnorm(Y4l, tst, iter, biases['decoder_b4'])\n",
    "    layer_4 = tf.nn.relu(Y4bn)\n",
    "    \n",
    "    global update_ema\n",
    "    update_ema = tf.group(update_ema5, update_ema6, update_ema7, update_ema8)\n",
    "    return layer_4\n",
    "\n",
    "\n",
    "\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "y_pred = decoder_op\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.pow(y_true-y_pred,2))\\\n",
    "    + l2_scale * (tf.nn.l2_loss(weights['encoder_h1'])+tf.nn.l2_loss(biases['encoder_b1']))\\\n",
    "    + (l1_regularizer(l1_scale, weights['encoder_h4'])+l1_regularizer(l1_scale, biases['encoder_b4']))\\\n",
    "    + (l1_regularizer(l1_scale, weights['decoder_h1'])+l1_regularizer(l1_scale, biases['decoder_b1']))\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "cost_summary = tf.summary.scalar('Cost', cost)  \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()  \n",
    "sess.run(init)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.random.permutation(train_data.shape[0])\n",
    "\n",
    "def get_batch_np(itert, batchsize):\n",
    "    batch_index = index[itert*batchsize:(itert+1)*batchsize]\n",
    "    batch_data = train_data[batch_index]\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "total_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 85.734306335\n",
      "Epoch: 0002 cost= 42.165725708\n",
      "Epoch: 0003 cost= 18.135400772\n",
      "Epoch: 0004 cost= 8.758343697\n",
      "Epoch: 0005 cost= 5.010951042\n",
      "Epoch: 0006 cost= 3.467998743\n",
      "Epoch: 0007 cost= 2.875286579\n",
      "Epoch: 0008 cost= 2.646712065\n",
      "Epoch: 0009 cost= 2.541959763\n",
      "Epoch: 0010 cost= 2.530435085\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500\n",
    "training_epochs = 10\n",
    "# 首先计算总批数，保证每次循环训练集中的每个样本都参与训练，不同于批量训练  \n",
    "total_batch = int(train_data.shape[0]/BATCH_SIZE) #总批数  \n",
    "\n",
    "merged_summary_op = tf.summary.merge([cost_summary])\n",
    "summary_writer = tf.summary.FileWriter('./tmp10/logs', sess.graph)\n",
    "\n",
    "for epoch in range(training_epochs):  \n",
    "    index = np.random.permutation(train_data.shape[0])\n",
    "    for i in range(total_batch):  \n",
    "        total_step += 1\n",
    "        batch_xs= get_batch_np(i, BATCH_SIZE)  # max(x) = 1, min(x) = 0  \n",
    "        \n",
    "        # Run optimization op (backprop) and cost op (to get loss value)  \n",
    "        summary_str, _, c = sess.run([merged_summary_op, optimizer, cost], feed_dict={X: batch_xs, learning_rate:1, tst: False, keep_prob:0.5})\n",
    "        sess.run([update_ema, update_emA], {X: batch_xs, tst: False, iter: total_step, keep_prob:0.5})\n",
    "        \n",
    "        summary_writer.add_summary(summary_str, total_step)\n",
    "        summary_writer.add_graph(sess.graph)\n",
    "        \n",
    "    if epoch % display_step == 0:  \n",
    "        record = open(\"train10_loss.txt\", \"a+\")\n",
    "        record.write(\"epoch:\"+str(epoch)+\" loss:\"+\"{:.9f}\\n\".format(c))\n",
    "        record.close()\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c)  \n",
    "#         init_op = tf.global_variables_initializer()\n",
    "#         saver = tf.train.Saver()\n",
    "#         with tf.Session() as sesstemp:\n",
    "#             sesstemp.run(init_op)\n",
    "#             saver_path = saver.save(sesstemp, \"model.ckpt\")\n",
    "#             print \"Model saved in file: \", saver_path\n",
    "print(\"Optimization Finished!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## classify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_class = 10\n",
    "\n",
    "y = tf.placeholder(\"float\", shape=[None, 10]) \n",
    "classify = tf.Variable(tf.truncated_normal([n_hidden_4, n_class], stddev=0.1), tf.float32, name='cf')\n",
    "weightse1_summary = tf.summary.histogram(\"weights_e1\", weights['encoder_h1'])\n",
    "weightse2_summary = tf.summary.histogram(\"weights_e2\", weights['encoder_h2'])\n",
    "weightse3_summary = tf.summary.histogram(\"weights_e3\", weights['encoder_h3'])\n",
    "weightse4_summary = tf.summary.histogram(\"weights_e4\", weights['encoder_h4'])\n",
    "cf_summary = tf.summary.histogram(\"weights_cf\", classify)\n",
    "\n",
    "def binaryclassify(x):\n",
    "    out = tf.matmul(x, classify)\n",
    "    return out\n",
    "\n",
    "# encoder_op = encoder(X)\n",
    "py = binaryclassify(encoder_op)\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(py), reduction_indices=[1]))\n",
    "# cross_entropy = -tf.reduce_mean(y*tf.log(tf.clip_by_value(py,1e-10,1.0)))\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=py))\\\n",
    "            + l2_scale * (tf.nn.l2_loss(weights['encoder_h1'])+tf.nn.l2_loss(biases['encoder_b1']))\\\n",
    "            + (l1_regularizer(l1_scale, weights['encoder_h4'])+l1_regularizer(l1_scale, biases['encoder_b4']))\\\n",
    "            + l1_regularizer(l1_scale, classify) \n",
    "# decoder_op = decoder(encoder_op)\n",
    "\n",
    "# cost = tf.reduce_mean(tf.pow(y_true-y_pred,2))\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "cost_summary = tf.summary.scalar('cross_entropy', cross_entropy)  \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(cross_entropy)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(py,1), tf.argmax(y, 1))\n",
    "trainacc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "trainacc_summary = tf.summary.scalar('trainacc', trainacc) \n",
    "\n",
    "testacc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "testacc_summary = tf.summary.scalar('testacc', testacc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CAPACITY = 30000\n",
    "# training_epochs = 10\n",
    "# learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "# display_step = 1\n",
    "# examples_to_show = 10\n",
    "# n_input = 22283 #Need to change to 22283\n",
    "# l1_scale = 0.001\n",
    "# l2_scale = 0.001\n",
    "\n",
    "# # tf Graph input (only pictures)\n",
    "\n",
    "# X = tf.placeholder(\"float\",[None,n_input])\n",
    "# y = tf.placeholder(\"float\", shape=[None, 2])  \n",
    "# # X = get_batch(data,BATCH_SIZE,CAPACITY)\n",
    "\n",
    "# #need to change to 4 layers \n",
    "# n_hidden_1 = 200\n",
    "# n_hidden_2 = 150\n",
    "# n_hidden_3 = 100\n",
    "# n_hidden_4 = 42\n",
    "# n_class = 2\n",
    "\n",
    "# weights= {\n",
    "#     'encoder_h1': tf.Variable(tf.truncated_normal([n_input,n_hidden_1], stddev=0.1), name='encoder_h1'),\n",
    "#     'encoder_h2': tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2], stddev=0.1), name='encoder_h2'),\n",
    "#     'encoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_3], stddev=0.1), name='encoder_h3'),\n",
    "#     'encoder_h4': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_4], stddev=0.1), name='encoder_h4'),\n",
    "#     'decoder_h1': tf.Variable(tf.truncated_normal([n_hidden_4,n_hidden_3], stddev=0.1), name='decoder_h1'),\n",
    "#     'decoder_h2': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_2], stddev=0.1), name='decoder_h2'),\n",
    "#     'decoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_1], stddev=0.1), name='decoder_h3'),\n",
    "#     'decoder_h4': tf.Variable(tf.truncated_normal([n_hidden_1,n_input], stddev=0.1), name='decoder_h4'),\n",
    "# }\n",
    "\n",
    "\n",
    "# classify = tf.Variable(tf.truncated_normal([n_hidden_4, n_class], stddev=0.1), tf.float32, name='cf')\n",
    "# biases = {\n",
    "#     'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1]), name='encoder_b1'),\n",
    "#     'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2]), name='encoder_b2'),\n",
    "#     'encoder_b3': tf.Variable(tf.random_normal([n_hidden_3]), name='encoder_b3'),\n",
    "#     'encoder_b4': tf.Variable(tf.random_normal([n_hidden_4]), name='encoder_b4'),\n",
    "#     'decoder_b1': tf.Variable(tf.random_normal([n_hidden_3]), name='decoder_b1'),\n",
    "#     'decoder_b2': tf.Variable(tf.random_normal([n_hidden_2]), name='decoder_b2'),\n",
    "#     'decoder_b3': tf.Variable(tf.random_normal([n_hidden_1]), name='decoder_b3'),\n",
    "#     'decoder_b4': tf.Variable(tf.random_normal([n_input]), name='decoder_b4'),\n",
    "# }\n",
    "\n",
    "# weightse1_summary = tf.summary.histogram(\"weights_e1\", weights['encoder_h1'])\n",
    "# weightse2_summary = tf.summary.histogram(\"weights_e2\", weights['encoder_h2'])\n",
    "# weightse3_summary = tf.summary.histogram(\"weights_e3\", weights['encoder_h3'])\n",
    "# weightse4_summary = tf.summary.histogram(\"weights_e4\", weights['encoder_h4'])\n",
    "# cf_summary = tf.summary.histogram(\"weights_cf\", classify)\n",
    "\n",
    "# # def encoder(x):  \n",
    "# layer_1 = tf.nn.relu(tf.add(tf.matmul(X, weights['encoder_h1']), biases['encoder_b1']))  \n",
    "# layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))  \n",
    "# layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['encoder_h3']), biases['encoder_b3']))\n",
    "# layer_4 = tf.nn.relu(tf.add(tf.matmul(layer_3, weights['encoder_h4']), biases['encoder_b4']))\n",
    "# #     return layer_4\n",
    "# #encoder_op = layer_4\n",
    "\n",
    "# def decoder(x):  \n",
    "#     layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['decoder_h4']),  \n",
    "#                                    biases['decoder_b4']))  \n",
    "# #     layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['decoder_h2']),  \n",
    "# #                                    biases['decoder_b2']))  \n",
    "# #     layer_3 = tf.nn.tanh(tf.add(tf.matmul(layer_2, weights['decoder_h3']),\n",
    "# #                                    biases['decoder_b3']))\n",
    "# #     layer_4 = tf.nn.tanh(tf.add(tf.matmul(layer_3, weights['decoder_h4']),\n",
    "# #                                    biases['decoder_b4']))\n",
    "#     return layer_1\n",
    "\n",
    "# def binaryclassify(x):\n",
    "#     out = tf.matmul(x, classify)\n",
    "#     return out\n",
    "\n",
    "# # encoder_op = encoder(X)\n",
    "# py = binaryclassify(encoder_op)\n",
    "# # cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(py), reduction_indices=[1]))\n",
    "# # cross_entropy = -tf.reduce_mean(y*tf.log(tf.clip_by_value(py,1e-10,1.0)))\n",
    "# cross_entropy = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=py))\\\n",
    "#             + l2_scale * (tf.nn.l2_loss(weights['encoder_h1'])+tf.nn.l2_loss(biases['encoder_b1']))\\\n",
    "#             + (l1_regularizer(l1_scale, weights['encoder_h4'])+l1_regularizer(l1_scale, biases['encoder_b4']))\\\n",
    "#             + l1_regularizer(l1_scale, classify)       \n",
    "# # decoder_op = decoder(encoder_op)\n",
    "\n",
    "# # y_pred = predict\n",
    "# # y_true = y\n",
    "\n",
    "# # cost = tf.reduce_mean(tf.pow(y_true-y_pred,2))\n",
    "# # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "# cost_summary = tf.summary.scalar('cross_entropy', cross_entropy)  \n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(py,1), tf.argmax(y, 1))\n",
    "# trainacc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# trainacc_summary = tf.summary.scalar('trainacc', trainacc) \n",
    "\n",
    "# testacc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# testacc_summary = tf.summary.scalar('testacc', testacc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() \n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "new = 1\n",
    "total_step = 0\n",
    "if new == 1:\n",
    "    sess.run(init)\n",
    "else:\n",
    "    init = tf.global_variables_initializer() \n",
    "    sess.run(init)\n",
    "    saver.restore(sess, 'model10.ckpt')\n",
    "    total_step = np.load(\"./total10_step.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# sess = tf.Session()\n",
    "# # new_saver = tf.train.import_meta_graph('model.ckpt.meta')\n",
    "# init = tf.global_variables_initializer() \n",
    "# sess.run(init)\n",
    "# saver.restore(sess, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_vars = tf.trainable_variables()\n",
    "# for v in all_vars:\n",
    "#     print v.name\n",
    "# sess.run(\"cf:0\")\n",
    "# #     sess.run(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## change test set to one-hot\n",
    "test_index = np.random.permutation(test_data.shape[0])\n",
    "tl_list = test_label[test_index]\n",
    "tl = np.zeros((test_label.shape[0], 10), dtype=np.int)\n",
    "line = 0\n",
    "td = test_data[test_index]\n",
    "for i in tl_list:\n",
    "    tl[line][i] = 1\n",
    "    line += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.random.permutation(train_data.shape[0])\n",
    "def get_batch_np(itert, batchsize):\n",
    "    batch_index = index[itert*batchsize:(itert+1)*batchsize]\n",
    "    batch_data = train_data[batch_index]\n",
    "    label_list = train_label[batch_index]\n",
    "    batch_label = np.zeros((batchsize, 10), dtype=np.int)\n",
    "    line = 0\n",
    "    for i in label_list:\n",
    "        batch_label[line][i] = 1\n",
    "        line += 1\n",
    "    return batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 19.795564651 trainacc =  0.068\n",
      "TestAcc =  0.045\n",
      "Epoch: 0002 cost= 19.526573181 trainacc =  0.076\n",
      "Epoch: 0003 cost= 19.220760345 trainacc =  0.094\n",
      "Epoch: 0004 cost= 18.981128693 trainacc =  0.094\n",
      "Epoch: 0005 cost= 18.696603775 trainacc =  0.136\n",
      "Epoch: 0006 cost= 18.446628571 trainacc =  0.152\n",
      "Epoch: 0007 cost= 18.179264069 trainacc =  0.166\n",
      "Epoch: 0008 cost= 17.910600662 trainacc =  0.200\n",
      "Epoch: 0009 cost= 17.674171448 trainacc =  0.204\n",
      "Epoch: 0010 cost= 17.466806412 trainacc =  0.204\n",
      "Epoch: 0011 cost= 17.193330765 trainacc =  0.258\n",
      "TestAcc =  0.154\n",
      "Epoch: 0012 cost= 16.918409348 trainacc =  0.310\n",
      "Epoch: 0013 cost= 16.715749741 trainacc =  0.306\n",
      "Epoch: 0014 cost= 16.465320587 trainacc =  0.318\n",
      "Epoch: 0015 cost= 16.264657974 trainacc =  0.334\n",
      "Epoch: 0016 cost= 16.005458832 trainacc =  0.352\n",
      "Epoch: 0017 cost= 15.797198296 trainacc =  0.346\n",
      "Epoch: 0018 cost= 15.516515732 trainacc =  0.418\n",
      "Epoch: 0019 cost= 15.358511925 trainacc =  0.376\n",
      "Epoch: 0020 cost= 15.163107872 trainacc =  0.348\n",
      "Epoch: 0021 cost= 14.936507225 trainacc =  0.358\n",
      "TestAcc =  0.230\n",
      "Epoch: 0022 cost= 14.736564636 trainacc =  0.402\n",
      "Epoch: 0023 cost= 14.551177025 trainacc =  0.408\n",
      "Epoch: 0024 cost= 14.348403931 trainacc =  0.392\n",
      "Epoch: 0025 cost= 14.154207230 trainacc =  0.400\n",
      "Epoch: 0026 cost= 13.957398415 trainacc =  0.400\n",
      "Epoch: 0027 cost= 13.813237190 trainacc =  0.358\n",
      "Epoch: 0028 cost= 13.570970535 trainacc =  0.428\n",
      "Epoch: 0029 cost= 13.382908821 trainacc =  0.406\n",
      "Epoch: 0030 cost= 13.255713463 trainacc =  0.402\n",
      "Epoch: 0031 cost= 12.998641014 trainacc =  0.452\n",
      "TestAcc =  0.253\n",
      "Epoch: 0032 cost= 12.841089249 trainacc =  0.434\n",
      "Epoch: 0033 cost= 12.671894073 trainacc =  0.430\n",
      "Epoch: 0034 cost= 12.547226906 trainacc =  0.400\n",
      "Epoch: 0035 cost= 12.318041801 trainacc =  0.434\n",
      "Epoch: 0036 cost= 12.208234787 trainacc =  0.404\n",
      "Epoch: 0037 cost= 12.020304680 trainacc =  0.410\n",
      "Epoch: 0038 cost= 11.869831085 trainacc =  0.428\n",
      "Epoch: 0039 cost= 11.761025429 trainacc =  0.416\n",
      "Epoch: 0040 cost= 11.579496384 trainacc =  0.380\n",
      "Epoch: 0041 cost= 11.374428749 trainacc =  0.436\n",
      "TestAcc =  0.240\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500\n",
    "training_epochs = 100\n",
    "test_display_step = 10\n",
    "# 首先计算总批数，保证每次循环训练集中的每个样本都参与训练，不同于批量训练  \n",
    "total_batch = int(train_data.shape[0]/BATCH_SIZE) #总批数  \n",
    " \n",
    "merged_summary_op = tf.summary.merge([cost_summary, weightse1_summary, weightse2_summary, weightse3_summary, weightse4_summary,\n",
    "                                     trainacc_summary, cf_summary])\n",
    "summary_writer = tf.summary.FileWriter('./tmp10/logs', sess.graph)\n",
    "summary_writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "for epoch in range(training_epochs):  \n",
    "    index = np.random.permutation(train_data.shape[0])\n",
    "    for i in range(total_batch):  \n",
    "        total_step += 1\n",
    "        batch_xs, batch_ys= get_batch_np(i, BATCH_SIZE)  # max(x) = 1, min(x) = 0  \n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        \n",
    "       # _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y: batch_ys, learning_rate:0.0001})\n",
    "        \n",
    "        summary_str, _, c, ta = sess.run([merged_summary_op, optimizer, cross_entropy, trainacc], \n",
    "                                               feed_dict={X: batch_xs, y: batch_ys, learning_rate:0.0001, tst: False, keep_prob:0.5})\n",
    "        sess.run([update_ema, update_emA], {X: batch_xs, y: batch_ys, tst: False, iter: total_step, keep_prob:0.5})\n",
    "        summary_writer.add_summary(summary_str, total_step)\n",
    "        \n",
    "    if epoch % display_step == 0:  \n",
    "        record = open(\"train10_loss.txt\", \"a+\")\n",
    "        record.write(\"epoch:\"+str(epoch)+\" loss:\"+\"{:.9f}\\n\".format(c))\n",
    "        record.close()\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c) , \"trainacc = \", \"{:.3f}\".format(ta)\n",
    "    if epoch % test_display_step == 0:\n",
    "        summary_str, tsta = sess.run([tf.summary.merge([testacc_summary]), testacc], feed_dict={X: td, y: tl, tst: False, keep_prob:0.5})\n",
    "        summary_writer.add_summary(summary_str, total_step)\n",
    "        print \"TestAcc = \", \"{:.3f}\".format(tsta)\n",
    "#         correct_prediction = tf.equal(tf.argmax(py,1), tf.cast(y, tf.int64))\n",
    "        \n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#         batch_xs, batch_ys= get_batch_np(0, 5000)\n",
    "#         cross_entropy = -tf.reduce_mean(y*tf.log(py))\n",
    "        # cross_entropy2 = -tf.reduce_mean(tf.cast(y, tf.int64)*tf.log(py))\n",
    "#         print(sess.run([accuracy, cross_entropy], feed_dict={X: test_data, y: test_label.reshape(1180,1)}))\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# saver = tf.train.Saver()\n",
    "saver_path = saver.save(sess, \"model10.ckpt\")\n",
    "print \"Model saved in file: \", saver_path\n",
    "np.save(\"total10_step\", total_step)\n",
    "print(\"Optimization Finished!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training set acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98533332]\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(py,1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "batch_xs, batch_ys= get_batch_np(0, 3000)\n",
    "# cross_entropy2 = -tf.reduce_mean(tf.cast(y, tf.int64)*tf.log(py))\n",
    "print(sess.run([accuracy], feed_dict={X: batch_xs, y: batch_ys}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.96866667]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.96866667]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test set acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92966104]\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(py,1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# batch_xs, batch_ys= get_batch_np(0, 4000)\n",
    "# cross_entropy2 = -tf.reduce_mean(tf.cast(y, tf.int64)*tf.log(py))\n",
    "print(sess.run([accuracy], feed_dict={X: td, y: tl, tst: False, keep_prob:1}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[0.93220341]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-28-707b93bbc41e>:27: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "[[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]]\n",
      "[[  1.74958725e-02   1.74958725e-02   1.74958725e-02   1.74958725e-02\n",
      "    1.74958725e-02   1.74958725e-02   1.74958725e-02   5.23108721e-01\n",
      "    1.74958725e-02   3.36924314e-01]\n",
      " [  1.90278515e-04   1.90278515e-04   1.90278515e-04   1.90278515e-04\n",
      "    1.90278515e-04   1.90278515e-04   1.90278515e-04   1.68166868e-02\n",
      "    1.90278515e-04   9.81661081e-01]]\n",
      "[[  1.77685414e-02   5.68983814e-05   5.68983814e-05   5.68983814e-05\n",
      "    5.68983814e-05   5.68983814e-05   5.68983814e-05   8.84178877e-01\n",
      "    9.21878219e-02   5.52334916e-03]\n",
      " [  6.64212108e-02   6.22349384e-04   6.22349384e-04   6.22349384e-04\n",
      "    6.22349384e-04   6.22349384e-04   6.22349384e-04   1.30644768e-01\n",
      "    7.98876345e-01   3.23597662e-04]]\n",
      "[[  5.18866256e-03   1.24175393e-04   1.24175393e-04   1.24175393e-04\n",
      "    1.24175393e-04   1.24175393e-04   1.24175393e-04   9.90048528e-01\n",
      "    3.84581345e-03   1.71974651e-04]\n",
      " [  8.08557034e-01   1.56743452e-03   1.56743452e-03   1.56743452e-03\n",
      "    1.56743452e-03   1.56743452e-03   1.56743452e-03   6.46150708e-02\n",
      "    1.17416456e-01   6.80501535e-06]]\n",
      "[[  9.65077698e-01   3.28526357e-06   3.48679163e-02   3.28526357e-06\n",
      "    3.28526357e-06   3.28526357e-06   3.28526357e-06   7.98589639e-08\n",
      "    3.78241712e-05   6.56856525e-09]\n",
      " [  4.82075120e-06   6.21139725e-06   9.60010570e-03   6.21139725e-06\n",
      "    6.21139725e-06   6.21139725e-06   6.21139725e-06   3.18950219e-06\n",
      "    2.81674384e-05   9.90332544e-01]]\n",
      "[0.221, array([10000,    10], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "#download teh MNIST data in folder \"MNIST_data\" that in the same path as this *.py\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#图片的占位\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "#系数\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#softmax层\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "#用于训练的真实值占位\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#交叉熵：-tf.reduce_sum(y_ * tf.log(y)是一个样本的，外面的tf.reduce_mean是batch的\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "#规定训练的方法：注意：使用GradientDescentOptimizer适合上述的误差项\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "#初始化\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#训练\n",
    "for i in range(5):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(2)\n",
    "  #print batch_xs.shape\n",
    "  a,_ = sess.run([y, train_step], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "  print a\n",
    "\n",
    "#验证，argmax(y,1)是获得y的第一个维度（即每一行）的最大值的位置\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run([accuracy,tf.shape(y)], feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
