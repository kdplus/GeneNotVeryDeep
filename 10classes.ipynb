{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import pickle as p\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.ops import standard_ops\n",
    "%matplotlib inline\n",
    "\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=False)\n",
    "\n",
    "def strip_first_col(fname, delimiter=None):\n",
    "    with open(fname, 'rb') as fin:\n",
    "        for line in fin:\n",
    "            try:\n",
    "               yield line.split(delimiter, 1)[1]\n",
    "            except IndexError:\n",
    "               continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load(\"./train9_data.npy\")\n",
    "train_label = np.load(\"./train9_label.npy\")\n",
    "test_data = np.load(\"./test9_data.npy\")\n",
    "test_label = np.load(\"./test9_label.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoencoder pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def l1_regularizer(scale, x):\n",
    "    return standard_ops.multiply(scale, standard_ops.reduce_sum(standard_ops.abs(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n",
    "    exp_moving_avg = tf.train.ExponentialMovingAverage(0.999, iteration) # adding the iteration prevents from averaging across non-existing iterations\n",
    "    bnepsilon = 1e-5\n",
    "    if convolutional:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0, 1, 2])\n",
    "    else:\n",
    "        mean, variance = tf.nn.moments(Ylogits, [0])\n",
    "    update_moving_everages = exp_moving_avg.apply([mean, variance])\n",
    "    m = tf.cond(is_test, lambda: exp_moving_avg.average(mean), lambda: mean)\n",
    "    v = tf.cond(is_test, lambda: exp_moving_avg.average(variance), lambda: variance)\n",
    "    Ybn = tf.nn.batch_normalization(Ylogits, m, v, offset, None, bnepsilon)\n",
    "    return Ybn, update_moving_everages\n",
    "\n",
    "def no_batchnorm(Ylogits, is_test, iteration, offset, convolutional=False):\n",
    "    return Ylogits, tf.no_op()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CAPACITY = 30000\n",
    "training_epochs = 10\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "n_input = 22283 #Need to change to 22283\n",
    "l1_scale = 0.001\n",
    "l2_scale = 0.001\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\",[None,n_input])\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "# X = get_batch(data,BATCH_SIZE,CAPACITY)\n",
    "tst = tf.placeholder(tf.bool)\n",
    "# training iteration\n",
    "iter = tf.placeholder(tf.int32)\n",
    "keep_prob = tf.placeholder(tf.float32)  \n",
    "\n",
    "#need to change to 4 layers \n",
    "n_hidden_1 = 200\n",
    "n_hidden_2 = 150\n",
    "n_hidden_3 = 100\n",
    "n_hidden_4 = 42\n",
    "\n",
    "weights= {\n",
    "    'encoder_h1': tf.Variable(tf.truncated_normal([n_input,n_hidden_1], stddev=0.1)),\n",
    "    'encoder_h2': tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2], stddev=0.1)),\n",
    "    'encoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_3], stddev=0.1)),\n",
    "    'encoder_h4': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_4], stddev=0.1)),\n",
    "    'decoder_h1': tf.Variable(tf.truncated_normal([n_hidden_4,n_hidden_3], stddev=0.1)),\n",
    "    'decoder_h2': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_2], stddev=0.1)),\n",
    "    'decoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_1], stddev=0.1)),\n",
    "    'decoder_h4': tf.Variable(tf.truncated_normal([n_hidden_1,n_input], stddev=0.1)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'encoder_b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'encoder_b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b3': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b4': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "# weightse1_summary = tf.summary.histogram(\"weights_e1\", weights['encoder_h1'])\n",
    "# weightse2_summary = tf.summary.histogram(\"weights_e2\", weights['encoder_h2'])\n",
    "# weightse3_summary = tf.summary.histogram(\"weights_e3\", weights['encoder_h3'])\n",
    "# weightsd1_summary = tf.summary.histogram(\"weights_d1\", weights['decoder_h1'])\n",
    "# weightsd2_summary = tf.summary.histogram(\"weights_d2\", weights['decoder_h1'])\n",
    "# weightsd3_summary = tf.summary.histogram(\"weights_d3\", weights['decoder_h1'])\n",
    "\n",
    "update_ema = []\n",
    "update_emA = []\n",
    "def encoder(x):  \n",
    "    Y1l = tf.add(tf.matmul(x, weights['encoder_h1']),biases['encoder_b1'])\n",
    "    Y1bn, update_ema1 = batchnorm(Y1l, tst, iter, biases['encoder_b1'])\n",
    "    layer_1 = tf.nn.relu(tf.nn.dropout(Y1bn, keep_prob))\n",
    "    \n",
    "    Y2l = tf.add(tf.matmul(layer_1, weights['encoder_h2']),biases['encoder_b2'])\n",
    "    Y2bn, update_ema2 = batchnorm(Y2l, tst, iter, biases['encoder_b2'])\n",
    "    layer_2 = tf.nn.relu(tf.nn.dropout(Y2bn, keep_prob))\n",
    "    \n",
    "    Y3l = tf.add(tf.matmul(layer_2, weights['encoder_h3']),biases['encoder_b3'])\n",
    "    Y3bn, update_ema3 = batchnorm(Y3l, tst, iter, biases['encoder_b3'])\n",
    "    layer_3 = tf.nn.relu(tf.nn.dropout(Y3bn, keep_prob))\n",
    "    \n",
    "    Y4l = tf.add(tf.matmul(layer_3, weights['encoder_h4']),biases['encoder_b4'])\n",
    "    Y4bn, update_ema4 = batchnorm(Y4l, tst, iter, biases['encoder_b4'])\n",
    "    layer_4 = tf.nn.relu(Y4bn)\n",
    "    \n",
    "    global update_ema\n",
    "    update_emA = tf.group(update_ema1, update_ema2, update_ema3, update_ema4)\n",
    "    return layer_4\n",
    "  \n",
    "def decoder(x):  \n",
    "#     layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['decoder_h1']),  biases['decoder_b1']))  \n",
    "#     layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2']))  \n",
    "#     layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['decoder_h3']),biases['decoder_b3']))\n",
    "#     layer_4 = tf.nn.relu(tf.add(tf.matmul(layer_3, weights['decoder_h4']),biases['decoder_b4']))\n",
    "    Y1l = tf.add(tf.matmul(x, weights['decoder_h1']),biases['decoder_b1'])\n",
    "    Y1bn, update_ema5 = batchnorm(Y1l, tst, iter, biases['decoder_b1'])\n",
    "    layer_1 = tf.nn.relu(tf.nn.dropout(Y1bn, keep_prob))\n",
    "    \n",
    "    Y2l = tf.add(tf.matmul(layer_1, weights['decoder_h2']),biases['decoder_b2'])\n",
    "    Y2bn, update_ema6 = batchnorm(Y2l, tst, iter, biases['decoder_b2'])\n",
    "    layer_2 = tf.nn.relu(tf.nn.dropout(Y2bn, keep_prob))\n",
    "\n",
    "    Y3l = tf.add(tf.matmul(layer_2, weights['decoder_h3']),biases['decoder_b3'])\n",
    "    Y3bn, update_ema7 = batchnorm(Y3l, tst, iter, biases['decoder_b3'])\n",
    "    layer_3 = tf.nn.relu(tf.nn.dropout(Y3bn, keep_prob))\n",
    "    \n",
    "    Y4l = tf.add(tf.matmul(layer_3, weights['decoder_h4']),biases['decoder_b4'])\n",
    "    Y4bn, update_ema8 = batchnorm(Y4l, tst, iter, biases['decoder_b4'])\n",
    "    layer_4 = tf.nn.relu(Y4bn)\n",
    "    \n",
    "    global update_ema\n",
    "    update_ema = tf.group(update_ema5, update_ema6, update_ema7, update_ema8)\n",
    "    return layer_4\n",
    "\n",
    "\n",
    "\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "y_pred = decoder_op\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.pow(y_true-y_pred,2))\\\n",
    "    + l2_scale * (tf.nn.l2_loss(weights['encoder_h1'])+tf.nn.l2_loss(biases['encoder_b1']))\\\n",
    "    + (l1_regularizer(l1_scale, weights['encoder_h4'])+l1_regularizer(l1_scale, biases['encoder_b4']))\\\n",
    "    + (l1_regularizer(l1_scale, weights['decoder_h1'])+l1_regularizer(l1_scale, biases['decoder_b1']))\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "cost_summary = tf.summary.scalar('Cost', cost)  \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(cost)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()  \n",
    "sess.run(init)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.random.permutation(train_data.shape[0])\n",
    "\n",
    "def get_batch_np(itert, batchsize):\n",
    "    batch_index = index[itert*batchsize:(itert+1)*batchsize]\n",
    "    batch_data = train_data[batch_index]\n",
    "    return batch_data\n",
    "\n",
    "\n",
    "total_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 47.990432739\n",
      "Epoch: 0002 cost= 25.637159348\n",
      "Epoch: 0003 cost= 15.693012238\n",
      "Epoch: 0004 cost= 11.684654236\n",
      "Epoch: 0005 cost= 8.763208389\n",
      "Epoch: 0006 cost= 5.366212368\n",
      "Epoch: 0007 cost= 3.705223083\n",
      "Epoch: 0008 cost= 5.816715240\n",
      "Epoch: 0009 cost= 6.495581627\n",
      "Epoch: 0010 cost= 5.312657833\n",
      "Epoch: 0011 cost= 4.318672657\n",
      "Epoch: 0012 cost= 3.715400219\n",
      "Epoch: 0013 cost= 3.302925825\n",
      "Epoch: 0014 cost= 3.031383991\n",
      "Epoch: 0015 cost= 4.000073910\n",
      "Epoch: 0016 cost= 3.512059927\n",
      "Epoch: 0017 cost= 11.580018997\n",
      "Epoch: 0018 cost= 7.119896889\n",
      "Epoch: 0019 cost= 4.833096027\n",
      "Epoch: 0020 cost= 3.715679884\n",
      "Epoch: 0021 cost= 3.210783005\n",
      "Epoch: 0022 cost= 75.264564514\n",
      "Epoch: 0023 cost= 21.762910843\n",
      "Epoch: 0024 cost= 11.404541016\n",
      "Epoch: 0025 cost= 6.940008163\n",
      "Epoch: 0026 cost= 4.860032082\n",
      "Epoch: 0027 cost= 3.897659779\n",
      "Epoch: 0028 cost= 3.373540640\n",
      "Epoch: 0029 cost= 3.124885559\n",
      "Epoch: 0030 cost= 3.084707737\n",
      "Epoch: 0031 cost= 3.014100552\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-852284a8f574>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Run optimization op (backprop) and cost op (to get loss value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0msummary_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged_summary_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mupdate_ema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_emA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtst\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtotal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    956\u001b[0m           if (not is_tensor_handle_feed and\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/core/numeric.pyc\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \"\"\"\n\u001b[0;32m--> 531\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500\n",
    "training_epochs = 100\n",
    "# 首先计算总批数，保证每次循环训练集中的每个样本都参与训练，不同于批量训练  \n",
    "total_batch = int(train_data.shape[0]/BATCH_SIZE) #总批数  \n",
    "\n",
    "merged_summary_op = tf.summary.merge([cost_summary])\n",
    "summary_writer = tf.summary.FileWriter('./tmp9/logs', sess.graph)\n",
    "\n",
    "for epoch in range(training_epochs):  \n",
    "    index = np.random.permutation(train_data.shape[0])\n",
    "    for i in range(total_batch):  \n",
    "        total_step += 1\n",
    "        batch_xs= get_batch_np(i, BATCH_SIZE)  # max(x) = 1, min(x) = 0  \n",
    "        \n",
    "        # Run optimization op (backprop) and cost op (to get loss value)  \n",
    "        summary_str, _, c = sess.run([merged_summary_op, optimizer, cost], feed_dict={X: batch_xs, learning_rate:1, tst: False, keep_prob:0.5})\n",
    "        sess.run([update_ema, update_emA], {X: batch_xs, tst: False, iter: total_step, keep_prob:0.5})\n",
    "        \n",
    "        summary_writer.add_summary(summary_str, total_step)\n",
    "        summary_writer.add_graph(sess.graph)\n",
    "        \n",
    "    if epoch % display_step == 0:  \n",
    "        record = open(\"train9_loss.txt\", \"a+\")\n",
    "        record.write(\"epoch:\"+str(epoch)+\" loss:\"+\"{:.9f}\\n\".format(c))\n",
    "        record.close()\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c)  \n",
    "#         init_op = tf.global_variables_initializer()\n",
    "#         saver = tf.train.Saver()\n",
    "#         with tf.Session() as sesstemp:\n",
    "#             sesstemp.run(init_op)\n",
    "#             saver_path = saver.save(sesstemp, \"model.ckpt\")\n",
    "#             print \"Model saved in file: \", saver_path\n",
    "print(\"Optimization Finished!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## classify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_class = 9\n",
    "\n",
    "y = tf.placeholder(\"float\", shape=[None, 9]) \n",
    "classify = tf.Variable(tf.truncated_normal([n_hidden_4, n_class], stddev=0.1), tf.float32, name='cf')\n",
    "weightse1_summary = tf.summary.histogram(\"weights_e1\", weights['encoder_h1'])\n",
    "weightse2_summary = tf.summary.histogram(\"weights_e2\", weights['encoder_h2'])\n",
    "weightse3_summary = tf.summary.histogram(\"weights_e3\", weights['encoder_h3'])\n",
    "weightse4_summary = tf.summary.histogram(\"weights_e4\", weights['encoder_h4'])\n",
    "cf_summary = tf.summary.histogram(\"weights_cf\", classify)\n",
    "\n",
    "def binaryclassify(x):\n",
    "    out = tf.matmul(x, classify)\n",
    "    return out\n",
    "\n",
    "# encoder_op = encoder(X)\n",
    "py = binaryclassify(encoder_op)\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(py), reduction_indices=[1]))\n",
    "# cross_entropy = -tf.reduce_mean(y*tf.log(tf.clip_by_value(py,1e-10,1.0)))\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=py))\\\n",
    "            + l2_scale * (tf.nn.l2_loss(weights['encoder_h1'])+tf.nn.l2_loss(biases['encoder_b1']))\\\n",
    "            + (l1_regularizer(l1_scale, weights['encoder_h4'])+l1_regularizer(l1_scale, biases['encoder_b4']))\\\n",
    "            + l1_regularizer(l1_scale, classify) \n",
    "# decoder_op = decoder(encoder_op)\n",
    "\n",
    "# cost = tf.reduce_mean(tf.pow(y_true-y_pred,2))\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "cost_summary = tf.summary.scalar('cross_entropy', cross_entropy)  \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(cross_entropy)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(py,1), tf.argmax(y, 1))\n",
    "trainacc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "trainacc_summary = tf.summary.scalar('trainacc', trainacc) \n",
    "\n",
    "testacc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "testacc_summary = tf.summary.scalar('testacc', testacc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CAPACITY = 30000\n",
    "# training_epochs = 10\n",
    "# learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "# display_step = 1\n",
    "# examples_to_show = 10\n",
    "# n_input = 22283 #Need to change to 22283\n",
    "# l1_scale = 0.001\n",
    "# l2_scale = 0.001\n",
    "\n",
    "# # tf Graph input (only pictures)\n",
    "\n",
    "# X = tf.placeholder(\"float\",[None,n_input])\n",
    "# y = tf.placeholder(\"float\", shape=[None, 2])  \n",
    "# # X = get_batch(data,BATCH_SIZE,CAPACITY)\n",
    "\n",
    "# #need to change to 4 layers \n",
    "# n_hidden_1 = 200\n",
    "# n_hidden_2 = 150\n",
    "# n_hidden_3 = 100\n",
    "# n_hidden_4 = 42\n",
    "# n_class = 2\n",
    "\n",
    "# weights= {\n",
    "#     'encoder_h1': tf.Variable(tf.truncated_normal([n_input,n_hidden_1], stddev=0.1), name='encoder_h1'),\n",
    "#     'encoder_h2': tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2], stddev=0.1), name='encoder_h2'),\n",
    "#     'encoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_3], stddev=0.1), name='encoder_h3'),\n",
    "#     'encoder_h4': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_4], stddev=0.1), name='encoder_h4'),\n",
    "#     'decoder_h1': tf.Variable(tf.truncated_normal([n_hidden_4,n_hidden_3], stddev=0.1), name='decoder_h1'),\n",
    "#     'decoder_h2': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_2], stddev=0.1), name='decoder_h2'),\n",
    "#     'decoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_1], stddev=0.1), name='decoder_h3'),\n",
    "#     'decoder_h4': tf.Variable(tf.truncated_normal([n_hidden_1,n_input], stddev=0.1), name='decoder_h4'),\n",
    "# }\n",
    "\n",
    "\n",
    "# classify = tf.Variable(tf.truncated_normal([n_hidden_4, n_class], stddev=0.1), tf.float32, name='cf')\n",
    "# biases = {\n",
    "#     'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1]), name='encoder_b1'),\n",
    "#     'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2]), name='encoder_b2'),\n",
    "#     'encoder_b3': tf.Variable(tf.random_normal([n_hidden_3]), name='encoder_b3'),\n",
    "#     'encoder_b4': tf.Variable(tf.random_normal([n_hidden_4]), name='encoder_b4'),\n",
    "#     'decoder_b1': tf.Variable(tf.random_normal([n_hidden_3]), name='decoder_b1'),\n",
    "#     'decoder_b2': tf.Variable(tf.random_normal([n_hidden_2]), name='decoder_b2'),\n",
    "#     'decoder_b3': tf.Variable(tf.random_normal([n_hidden_1]), name='decoder_b3'),\n",
    "#     'decoder_b4': tf.Variable(tf.random_normal([n_input]), name='decoder_b4'),\n",
    "# }\n",
    "\n",
    "# weightse1_summary = tf.summary.histogram(\"weights_e1\", weights['encoder_h1'])\n",
    "# weightse2_summary = tf.summary.histogram(\"weights_e2\", weights['encoder_h2'])\n",
    "# weightse3_summary = tf.summary.histogram(\"weights_e3\", weights['encoder_h3'])\n",
    "# weightse4_summary = tf.summary.histogram(\"weights_e4\", weights['encoder_h4'])\n",
    "# cf_summary = tf.summary.histogram(\"weights_cf\", classify)\n",
    "\n",
    "# # def encoder(x):  \n",
    "# layer_1 = tf.nn.relu(tf.add(tf.matmul(X, weights['encoder_h1']), biases['encoder_b1']))  \n",
    "# layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))  \n",
    "# layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['encoder_h3']), biases['encoder_b3']))\n",
    "# layer_4 = tf.nn.relu(tf.add(tf.matmul(layer_3, weights['encoder_h4']), biases['encoder_b4']))\n",
    "# #     return layer_4\n",
    "# #encoder_op = layer_4\n",
    "\n",
    "# def decoder(x):  \n",
    "#     layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['decoder_h4']),  \n",
    "#                                    biases['decoder_b4']))  \n",
    "# #     layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['decoder_h2']),  \n",
    "# #                                    biases['decoder_b2']))  \n",
    "# #     layer_3 = tf.nn.tanh(tf.add(tf.matmul(layer_2, weights['decoder_h3']),\n",
    "# #                                    biases['decoder_b3']))\n",
    "# #     layer_4 = tf.nn.tanh(tf.add(tf.matmul(layer_3, weights['decoder_h4']),\n",
    "# #                                    biases['decoder_b4']))\n",
    "#     return layer_1\n",
    "\n",
    "# def binaryclassify(x):\n",
    "#     out = tf.matmul(x, classify)\n",
    "#     return out\n",
    "\n",
    "# # encoder_op = encoder(X)\n",
    "# py = binaryclassify(encoder_op)\n",
    "# # cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(py), reduction_indices=[1]))\n",
    "# # cross_entropy = -tf.reduce_mean(y*tf.log(tf.clip_by_value(py,1e-10,1.0)))\n",
    "# cross_entropy = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=py))\\\n",
    "#             + l2_scale * (tf.nn.l2_loss(weights['encoder_h1'])+tf.nn.l2_loss(biases['encoder_b1']))\\\n",
    "#             + (l1_regularizer(l1_scale, weights['encoder_h4'])+l1_regularizer(l1_scale, biases['encoder_b4']))\\\n",
    "#             + l1_regularizer(l1_scale, classify)       \n",
    "# # decoder_op = decoder(encoder_op)\n",
    "\n",
    "# # y_pred = predict\n",
    "# # y_true = y\n",
    "\n",
    "# # cost = tf.reduce_mean(tf.pow(y_true-y_pred,2))\n",
    "# # cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "# cost_summary = tf.summary.scalar('cross_entropy', cross_entropy)  \n",
    "# optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(cross_entropy)\n",
    "\n",
    "\n",
    "# correct_prediction = tf.equal(tf.argmax(py,1), tf.argmax(y, 1))\n",
    "# trainacc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# trainacc_summary = tf.summary.scalar('trainacc', trainacc) \n",
    "\n",
    "# testacc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# testacc_summary = tf.summary.scalar('testacc', testacc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer() \n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "new = 1\n",
    "total_step = 0\n",
    "if new == 1:\n",
    "    sess.run(init)\n",
    "else:\n",
    "    init = tf.global_variables_initializer() \n",
    "    sess.run(init)\n",
    "    saver.restore(sess, 'model9.ckpt')\n",
    "    total_step = np.load(\"./total9_step.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# sess = tf.Session()\n",
    "# # new_saver = tf.train.import_meta_graph('model.ckpt.meta')\n",
    "# init = tf.global_variables_initializer() \n",
    "# sess.run(init)\n",
    "# saver.restore(sess, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_vars = tf.trainable_variables()\n",
    "# for v in all_vars:\n",
    "#     print v.name\n",
    "# sess.run(\"cf:0\")\n",
    "# #     sess.run(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## change test set to one-hot\n",
    "test_index = np.random.permutation(test_data.shape[0])\n",
    "tl_list = test_label[test_index]\n",
    "tl = np.zeros((test_label.shape[0], 9), dtype=np.int)\n",
    "line = 0\n",
    "td = test_data[test_index]\n",
    "for i in tl_list:\n",
    "    tl[line][i] = 1\n",
    "    line += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.random.permutation(train_data.shape[0])\n",
    "def get_batch_np(itert, batchsize):\n",
    "    batch_index = index[itert*batchsize:(itert+1)*batchsize]\n",
    "    batch_data = train_data[batch_index]\n",
    "    label_list = train_label[batch_index]\n",
    "    batch_label = np.zeros((batchsize, 9), dtype=np.int)\n",
    "    line = 0\n",
    "    for i in label_list:\n",
    "        batch_label[line][i] = 1\n",
    "        line += 1\n",
    "    return batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_np_for_class(spe_class):\n",
    "    batch_index=[]\n",
    "    for i in range(len(train_label)):\n",
    "        if (train_label[i]==spe_class):\n",
    "            batch_index.append(i)\n",
    "    label_list = train_label[batch_index]\n",
    "    batch_data = train_data[batch_index]\n",
    "    batch_label = np.zeros((len(batch_index), 9), dtype=np.int)\n",
    "    line = 0\n",
    "    for i in label_list:\n",
    "        batch_label[line][i] = 1\n",
    "        line += 1\n",
    "    return batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.095371686 trainacc =  0.988\n",
      "TestAcc =  0.908\n",
      "Best Model saved in file:  best9_model.ckpt\n",
      "Epoch: 0002 cost= 0.098434262 trainacc =  0.986\n",
      "Epoch: 0003 cost= 0.090923093 trainacc =  0.996\n",
      "Epoch: 0004 cost= 0.095441788 trainacc =  0.996\n",
      "Epoch: 0005 cost= 0.096909575 trainacc =  0.986\n",
      "Epoch: 0006 cost= 0.096753471 trainacc =  0.984\n",
      "Epoch: 0007 cost= 0.087388210 trainacc =  0.998\n",
      "Epoch: 0008 cost= 0.095956296 trainacc =  0.988\n",
      "Epoch: 0009 cost= 0.104461253 trainacc =  0.976\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500\n",
    "training_epochs = 1000\n",
    "test_display_step = 10\n",
    "best_acc = 0.0\n",
    "#best_acc = np.load(\"./best9_acc.npy\")\n",
    "# 首先计算总批数，保证每次循环训练集中的每个样本都参与训练，不同于批量训练  \n",
    "total_batch = int(train_data.shape[0]/BATCH_SIZE) #总批数  \n",
    " \n",
    "merged_summary_op = tf.summary.merge([cost_summary, weightse1_summary, weightse2_summary, weightse3_summary, weightse4_summary,\n",
    "                                     trainacc_summary, cf_summary])\n",
    "summary_writer = tf.summary.FileWriter('./tmp9/logs', sess.graph)\n",
    "summary_writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "for epoch in range(training_epochs):  \n",
    "    index = np.random.permutation(train_data.shape[0])\n",
    "    for i in range(total_batch):  \n",
    "        total_step += 1\n",
    "        batch_xs, batch_ys= get_batch_np(i, BATCH_SIZE)  # max(x) = 1, min(x) = 0  \n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        \n",
    "       # _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y: batch_ys, learning_rate:0.0001})\n",
    "        \n",
    "        summary_str, _, c, ta = sess.run([merged_summary_op, optimizer, cross_entropy, trainacc], \n",
    "                                               feed_dict={X: batch_xs, y: batch_ys, learning_rate:0.0001, tst: False, keep_prob:0.8})\n",
    "        sess.run([update_ema, update_emA], {X: batch_xs, y: batch_ys, tst: False, iter: total_step, keep_prob:0.4})\n",
    "        summary_writer.add_summary(summary_str, total_step)\n",
    "        \n",
    "    if epoch % display_step == 0:  \n",
    "        record = open(\"train9_loss.txt\", \"a+\")\n",
    "        record.write(\"epoch:\"+str(epoch)+\" loss:\"+\"{:.9f}\\n\".format(c))\n",
    "        record.close()\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c) , \"trainacc = \", \"{:.3f}\".format(ta)\n",
    "    if epoch % test_display_step == 0:\n",
    "        summary_str, tsta = sess.run([tf.summary.merge([testacc_summary]), testacc], feed_dict={X: td, y: tl, tst: False, keep_prob:1.0})\n",
    "        summary_writer.add_summary(summary_str, total_step)\n",
    "        print \"TestAcc = \", \"{:.3f}\".format(tsta)\n",
    "        if tsta > best_acc:\n",
    "            best_acc = tsta\n",
    "            saver_path = saver.save(sess, \"best9_model.ckpt\")\n",
    "            print \"Best Model saved in file: \", saver_path\n",
    "            np.save(\"total_step\", total_step)\n",
    "            np.save(\"best9_acc\", best_acc)\n",
    "#         correct_prediction = tf.equal(tf.argmax(py,1), tf.cast(y, tf.int64))\n",
    "        \n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#         batch_xs, batch_ys= get_batch_np(0, 5000)\n",
    "#         cross_entropy = -tf.reduce_mean(y*tf.log(py))\n",
    "        # cross_entropy2 = -tf.reduce_mean(tf.cast(y, tf.int64)*tf.log(py))\n",
    "#         print(sess.run([accuracy, cross_entropy], feed_dict={X: test_data, y: test_label.reshape(1180,1)}))\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# saver = tf.train.Saver()\n",
    "saver_path = saver.save(sess, \"model9.ckpt\")\n",
    "print \"Model saved in file: \", saver_path\n",
    "np.save(\"total9_step\", total_step)\n",
    "print(\"Optimization Finished!\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(0.9084745645523071, dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "3+3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training set acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(py,1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "batch_xs, batch_ys= get_batch_np(0, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_result=sess.run([tf.argmax(py,1)], feed_dict={X: batch_xs, y: batch_ys, tst: False, keep_prob:1})\n",
    "true_result=sess.run([tf.argmax(batch_ys,1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_result=test_result[0]\n",
    "true_result=true_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "[925, 1201, 257, 146, 0, 0, 0, 0, 0, 0]\n",
      "[77, 62, 7, 0, 85, 74, 65, 50, 51, 0]\n",
      "[56, 395, 20, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1002, 1263, 264, 146, 85, 74, 65, 50, 51, 0]\n"
     ]
    }
   ],
   "source": [
    "totaL=[0]*10\n",
    "TP=[0]*10\n",
    "FP=[0]*10\n",
    "FN=[0]*10\n",
    "for i in range(len(test_result)):\n",
    "    s=test_result[i]\n",
    "    t=true_result[i]\n",
    "    if (t==7):\n",
    "        print(s)\n",
    "    totaL[t]+=1\n",
    "    if (s==t):\n",
    "        TP[t]+=1\n",
    "    if (s!=t):\n",
    "        FP[t]+=1\n",
    "        FN[s]+=1\n",
    "print (TP)\n",
    "print (FP)\n",
    "print (FN)\n",
    "print (totaL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test set acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(py,1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# batch_xs, batch_ys= get_batch_np(0, 4000)\n",
    "# cross_entropy2 = -tf.reduce_mean(tf.cast(y, tf.int64)*tf.log(py))\n",
    "test_result,acc=sess.run([tf.argmax(py,1),accuracy], feed_dict={X: td, y: tl, tst: False, keep_prob:1})\n",
    "true_result=sess.run([tf.argmax(tl,1)])\n",
    "py_result=sess.run([py], feed_dict={X: td, y: tl, tst: False, keep_prob:1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "[0 1 0 ..., 0 8 1]\n",
      "0.912712\n"
     ]
    }
   ],
   "source": [
    "test_result=test_result[0]\n",
    "true_result=true_result[0]\n",
    "print test_result\n",
    "print true_result\n",
    "print acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.32635403,  6.73614311, -0.50485295, -0.43004325, -0.67035609,\n",
       "       -1.11145842, -1.01467097, -0.87309933, -1.07652509], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py_result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.78540528  1.15033185 -0.08763568  0.70137501 -0.08222654  0.70726204\n",
      " -0.51913953  8.9113245  -0.72438836]\n",
      "[ 0.76346916  1.08256698 -0.07724989  0.66791648 -0.09944928  0.70019561\n",
      " -0.57360125  8.63863182 -0.71990883]\n",
      "[ 0.61603332  1.15654123 -0.00719299  0.46083152 -0.04915589  0.40126169\n",
      " -0.84075743  6.96723843 -0.74477857]\n",
      "[ 0.90415394  1.04408216 -0.07750054  0.71873754 -0.08656622  0.61720085\n",
      " -0.4938488   8.73170948 -0.73863339]\n",
      "[ 0.81721878  1.09879148 -0.07352722  0.68984717 -0.10524125  0.67671669\n",
      " -0.61302406  8.76390839 -0.73331004]\n",
      "[ 0.75435954  1.09232926 -0.07489618  0.66636807 -0.10441831  0.7164529\n",
      " -0.55021101  8.66203594 -0.71150625]\n",
      "[ 0.68880749  0.99047446 -0.09054399  0.65642154 -0.07529666  0.77913654\n",
      " -0.41447604  8.528862   -0.66270661]\n",
      "[ 0.71261758  0.93442601 -0.12795623  0.65449262  0.01226631  0.6555692\n",
      " -0.20335233  8.19138241 -0.68058616]\n",
      "[ 0.77566236  1.01932275 -0.09289231  0.6336236  -0.06254087  0.64244884\n",
      " -0.54391199  8.32156754 -0.72487712]\n",
      "[ 0.81453121  1.11367214 -0.07726135  0.71965373 -0.09644651  0.66915053\n",
      " -0.55505657  8.87030792 -0.73520797]\n",
      "[ 0.80585128  1.0469867  -0.05778236  0.58686191 -0.07069485  0.36846304\n",
      " -0.80144668  7.5526948  -0.7988807 ]\n",
      "[ 0.81842524  1.13679147 -0.07942569  0.73318505 -0.1022464   0.76232785\n",
      " -0.49277717  9.16794109 -0.71002603]\n",
      "[ 0.78415912  1.09663999 -0.07269695  0.69792682 -0.11265311  0.74931574\n",
      " -0.55679673  8.9011755  -0.70622087]\n",
      "[ 0.57031548  0.94260246 -0.11201011  0.5639863  -0.0286425   0.73885763\n",
      " -0.38212419  7.89674377 -0.65126741]\n",
      "[ 0.8027702   1.08554673 -0.08381904  0.69985026 -0.087048    0.71642536\n",
      " -0.48039055  8.84556293 -0.71223122]\n",
      "[ 0.77276868  1.07374692 -0.07610466  0.64577359 -0.09538927  0.63379037\n",
      " -0.64333445  8.41363239 -0.73845595]\n",
      "[ 0.84518665  1.10305965 -0.07536719  0.68204117 -0.09388218  0.60018778\n",
      " -0.60097378  8.60012054 -0.75957787]\n",
      "[ 0.78444093  1.05780804 -0.06673949  0.6707294  -0.10559331  0.58951652\n",
      " -0.63074362  8.36817646 -0.746571  ]\n",
      "[ 0.75790924  0.99303788 -0.10435335  0.62737161 -0.02917503  0.54794765\n",
      " -0.49933955  8.02435303 -0.74371403]\n",
      "[351, 437, 102, 64, 35, 25, 20, 19, 24, 0]\n",
      "[45, 50, 5, 0, 2, 1, 0, 0, 0, 0]\n",
      "[32, 52, 5, 0, 7, 0, 6, 1, 0, 0]\n",
      "[396, 487, 107, 64, 37, 26, 20, 19, 24, 0]\n"
     ]
    }
   ],
   "source": [
    "totaL=[0]*10\n",
    "TP=[0]*10\n",
    "FP=[0]*10\n",
    "FN=[0]*10\n",
    "for i in range(len(test_result)):\n",
    "    s=test_result[i]\n",
    "    t=true_result[i]\n",
    "    if (t==7):\n",
    "        print py_result[0][i]\n",
    "    totaL[t]+=1\n",
    "    if (s==t):\n",
    "        TP[t]+=1\n",
    "    if (s!=t):\n",
    "        FP[t]+=1\n",
    "        FN[s]+=1\n",
    "print (TP)\n",
    "print (FP)\n",
    "print (FN)\n",
    "print (totaL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision: 0.886363636364\n",
      "recall: 0.916449086162\n",
      "------------------------------------------------------------\n",
      "precision: 0.897330595483\n",
      "recall: 0.893660531697\n",
      "------------------------------------------------------------\n",
      "precision: 0.953271028037\n",
      "recall: 0.953271028037\n",
      "------------------------------------------------------------\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "------------------------------------------------------------\n",
      "precision: 0.945945945946\n",
      "recall: 0.833333333333\n",
      "------------------------------------------------------------\n",
      "precision: 0.961538461538\n",
      "recall: 1.0\n",
      "------------------------------------------------------------\n",
      "precision: 1.0\n",
      "recall: 0.769230769231\n",
      "------------------------------------------------------------\n",
      "precision: 1.0\n",
      "recall: 0.95\n",
      "------------------------------------------------------------\n",
      "precision: 1.0\n",
      "recall: 1.0\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(9):\n",
    "    print \"precision:\",TP[i]/float(TP[i]+FP[i])\n",
    "    print \"recall:\",TP[i]/float(TP[i]+FN[i])\n",
    "    print \"------------------------------------------------------------\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-28-707b93bbc41e>:27: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "[[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]]\n",
      "[[  1.74958725e-02   1.74958725e-02   1.74958725e-02   1.74958725e-02\n",
      "    1.74958725e-02   1.74958725e-02   1.74958725e-02   5.23108721e-01\n",
      "    1.74958725e-02   3.36924314e-01]\n",
      " [  1.90278515e-04   1.90278515e-04   1.90278515e-04   1.90278515e-04\n",
      "    1.90278515e-04   1.90278515e-04   1.90278515e-04   1.68166868e-02\n",
      "    1.90278515e-04   9.81661081e-01]]\n",
      "[[  1.77685414e-02   5.68983814e-05   5.68983814e-05   5.68983814e-05\n",
      "    5.68983814e-05   5.68983814e-05   5.68983814e-05   8.84178877e-01\n",
      "    9.21878219e-02   5.52334916e-03]\n",
      " [  6.64212108e-02   6.22349384e-04   6.22349384e-04   6.22349384e-04\n",
      "    6.22349384e-04   6.22349384e-04   6.22349384e-04   1.30644768e-01\n",
      "    7.98876345e-01   3.23597662e-04]]\n",
      "[[  5.18866256e-03   1.24175393e-04   1.24175393e-04   1.24175393e-04\n",
      "    1.24175393e-04   1.24175393e-04   1.24175393e-04   9.90048528e-01\n",
      "    3.84581345e-03   1.71974651e-04]\n",
      " [  8.08557034e-01   1.56743452e-03   1.56743452e-03   1.56743452e-03\n",
      "    1.56743452e-03   1.56743452e-03   1.56743452e-03   6.46150708e-02\n",
      "    1.17416456e-01   6.80501535e-06]]\n",
      "[[  9.65077698e-01   3.28526357e-06   3.48679163e-02   3.28526357e-06\n",
      "    3.28526357e-06   3.28526357e-06   3.28526357e-06   7.98589639e-08\n",
      "    3.78241712e-05   6.56856525e-09]\n",
      " [  4.82075120e-06   6.21139725e-06   9.60010570e-03   6.21139725e-06\n",
      "    6.21139725e-06   6.21139725e-06   6.21139725e-06   3.18950219e-06\n",
      "    2.81674384e-05   9.90332544e-01]]\n",
      "[0.221, array([10000,    10], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "#download teh MNIST data in folder \"MNIST_data\" that in the same path as this *.py\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#图片的占位\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "#系数\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#softmax层\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "#用于训练的真实值占位\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#交叉熵：-tf.reduce_sum(y_ * tf.log(y)是一个样本的，外面的tf.reduce_mean是batch的\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "#规定训练的方法：注意：使用GradientDescentOptimizer适合上述的误差项\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "#初始化\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#训练\n",
    "for i in range(5):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(2)\n",
    "  #print batch_xs.shape\n",
    "  a,_ = sess.run([y, train_step], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "  print a\n",
    "\n",
    "#验证，argmax(y,1)是获得y的第一个维度（即每一行）的最大值的位置\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run([accuracy,tf.shape(y)], feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
