{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle as p\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# from tensorflow.examples.tutorials.mnist import input_data\n",
    "# mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=False)\n",
    "\n",
    "def strip_first_col(fname, delimiter=None):\n",
    "    with open(fname, 'rb') as fin:\n",
    "        for line in fin:\n",
    "            try:\n",
    "               yield line.split(delimiter, 1)[1]\n",
    "            except IndexError:\n",
    "               continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = np.load(\"./train_data.npy\")\n",
    "train_label = np.load(\"./train_label.npy\")\n",
    "test_data = np.load(\"./test_data.npy\")\n",
    "test_label = np.load(\"./test_label.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## autoencoder pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CAPACITY = 30000\n",
    "training_epochs = 10\n",
    "learning_rate = 0.01\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "n_input = 22283 #Need to change to 22283\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "X = tf.placeholder(\"float\",[None,n_input])\n",
    "# X = get_batch(data,BATCH_SIZE,CAPACITY)\n",
    "\n",
    "#need to change to 4 layers \n",
    "n_hidden_1 = 1\n",
    "# n_hidden_2 = 22283\n",
    "# n_hidden_3 = 22283\n",
    "# n_hidden_4 = 22283\n",
    "\n",
    "weights= {\n",
    "    'encoder_h1': tf.get_variable('eh1', [n_input,n_hidden_1],tf.float32, initializer=tf.contrib.layers.xavier_initializer()),\n",
    "#     'encoder_h2': tf.get_variable('eh2', [n_hidden_1,n_hidden_2],tf.float32, initializer=tf.contrib.layers.xavier_initializer()),\n",
    "#     'encoder_h3': tf.get_variable('eh3', [n_hidden_2,n_hidden_3],tf.float32, initializer=tf.contrib.layers.xavier_initializer()),\n",
    "#     'encoder_h4': tf.get_variable('eh4', [n_hidden_3,n_hidden_4],tf.float32, initializer=tf.contrib.layers.xavier_initializer()),\n",
    "#     'decoder_h1': tf.get_variable('dh1', [n_hidden_4,n_hidden_3],tf.float32, initializer=tf.contrib.layers.xavier_initializer()),\n",
    "#     'decoder_h2': tf.get_variable('dh2', [n_hidden_3,n_hidden_2],tf.float32, initializer=tf.contrib.layers.xavier_initializer()),\n",
    "#     'decoder_h3': tf.get_variable('dh3', [n_hidden_2,n_hidden_1],tf.float32, initializer=tf.contrib.layers.xavier_initializer()),\n",
    "    'decoder_h4': tf.get_variable('dh4', [n_hidden_1,n_input],tf.float32, initializer=tf.contrib.layers.xavier_initializer()),\n",
    "#     'encoder_h2': tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2], stddev=0.1)),\n",
    "#     'encoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_3], stddev=0.1)),\n",
    "#     'encoder_h4': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_4], stddev=0.1)),\n",
    "#     'decoder_h1': tf.Variable(tf.truncated_normal([n_hidden_4,n_hidden_3], stddev=0.1)),\n",
    "#     'decoder_h2': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_2], stddev=0.1)),\n",
    "#     'decoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_1], stddev=0.1)),\n",
    "#     'decoder_h4': tf.Variable(tf.truncated_normal([n_hidden_1,n_input], stddev=0.1)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "#     'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "#     'encoder_b3': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "#     'encoder_b4': tf.Variable(tf.random_normal([n_hidden_4])),\n",
    "#     'decoder_b1': tf.Variable(tf.random_normal([n_hidden_3])),\n",
    "#     'decoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "#     'decoder_b3': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b4': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    "# weightse1_summary = tf.summary.histogram(\"weights_e1\", weights['encoder_h1'])\n",
    "# weightse2_summary = tf.summary.histogram(\"weights_e2\", weights['encoder_h2'])\n",
    "# weightse3_summary = tf.summary.histogram(\"weights_e3\", weights['encoder_h3'])\n",
    "# weightsd1_summary = tf.summary.histogram(\"weights_d1\", weights['decoder_h1'])\n",
    "# weightsd2_summary = tf.summary.histogram(\"weights_d2\", weights['decoder_h1'])\n",
    "# weightsd3_summary = tf.summary.histogram(\"weights_d3\", weights['decoder_h1'])\n",
    "\n",
    "def encoder(x):  \n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['encoder_h1']),  \n",
    "                                   biases['encoder_b1']))  \n",
    "#     layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['encoder_h2']),  \n",
    "#                                    biases['encoder_b2']))  \n",
    "#     layer_3 = tf.nn.tanh(tf.add(tf.matmul(layer_2, weights['encoder_h3']),\n",
    "#                                    biases['encoder_b3']))\n",
    "#     layer_4 = tf.nn.tanh(tf.add(tf.matmul(layer_3, weights['encoder_h4']),\n",
    "#                                    biases['encoder_b4']))\n",
    "    return layer_1\n",
    "\n",
    "def decoder(x):  \n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['decoder_h4']),  \n",
    "                                   biases['decoder_b4']))  \n",
    "#     layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['decoder_h2']),  \n",
    "#                                    biases['decoder_b2']))  \n",
    "#     layer_3 = tf.nn.tanh(tf.add(tf.matmul(layer_2, weights['decoder_h3']),\n",
    "#                                    biases['decoder_b3']))\n",
    "#     layer_4 = tf.nn.tanh(tf.add(tf.matmul(layer_3, weights['decoder_h4']),\n",
    "#                                    biases['decoder_b4']))\n",
    "    return layer_1\n",
    "\n",
    "encoder_op = encoder(X)\n",
    "decoder_op = decoder(encoder_op)\n",
    "\n",
    "y_pred = decoder_op\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(tf.pow(y_true-y_pred,2))\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "cost_summary = tf.summary.scalar('Cost', cost)  \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "sess = tf.Session()\n",
    "if int((tf.__version__).split('.')[1]) < 12 and int((tf.__version__).split('.')[0]) < 1:  \n",
    "    init = tf.initialize_all_variables()  \n",
    "else:  \n",
    "    init = tf.global_variables_initializer()  \n",
    "sess.run(init)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.random.permutation(raw_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batch_np(itert, batchsize):\n",
    "    batch_index = index[itert*batchsize:(itert+1)*batchsize]\n",
    "    batch_data = raw_data[batch_index]\n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0 cost = 1132690.875000000\n",
      "Iter: 1 cost = 1114782.625000000\n",
      "Iter: 2 cost = 1127686.625000000\n",
      "Iter: 3 cost = 1148600.500000000\n",
      "Iter: 4 cost = 1138921.375000000\n",
      "Iter: 5 cost = 1130688.500000000\n",
      "Iter: 6 cost = 1113523.750000000\n",
      "Iter: 7 cost = 1128154.625000000\n",
      "Iter: 8 cost = 1086702.375000000\n",
      "Iter: 9 cost = 1127517.375000000\n",
      "Iter: 10 cost = 1128266.500000000\n",
      "Iter: 20 cost = 1131665.625000000\n",
      "Iter: 30 cost = 1117142.125000000\n",
      "Iter: 40 cost = 1119001.500000000\n",
      "Iter: 50 cost = 1129301.250000000\n",
      "Iter: 60 cost = 1134886.125000000\n",
      "Iter: 70 cost = 1128734.875000000\n",
      "Iter: 80 cost = 1129180.875000000\n",
      "Iter: 90 cost = 1092207.375000000\n",
      "Iter: 100 cost = 1138906.250000000\n",
      "Iter: 110 cost = 1118319.875000000\n",
      "Iter: 120 cost = 1135590.625000000\n",
      "Iter: 130 cost = 1112696.375000000\n",
      "Iter: 140 cost = 1127809.500000000\n",
      "Iter: 150 cost = 1125737.500000000\n",
      "Iter: 160 cost = 1130493.375000000\n",
      "Iter: 170 cost = 1121293.000000000\n",
      "Iter: 180 cost = 1126222.000000000\n",
      "Iter: 190 cost = 1100796.375000000\n",
      "Iter: 200 cost = 1139084.875000000\n",
      "Iter: 210 cost = 1121286.250000000\n",
      "Iter: 220 cost = 1124581.500000000\n",
      "Iter: 230 cost = 1132862.750000000\n",
      "Iter: 240 cost = 1134124.125000000\n",
      "Iter: 250 cost = 1122145.500000000\n",
      "Iter: 260 cost = 1141845.250000000\n",
      "Iter: 270 cost = 1122243.750000000\n",
      "Iter: 280 cost = 1107826.625000000\n",
      "Iter: 290 cost = 1131793.250000000\n",
      "Iter: 300 cost = 1134099.250000000\n",
      "Iter: 310 cost = 1070248.125000000\n",
      "Iter: 320 cost = 1128135.625000000\n",
      "Iter: 330 cost = 1119545.500000000\n",
      "Iter: 340 cost = 1132939.625000000\n",
      "Iter: 350 cost = 1140072.875000000\n",
      "Iter: 360 cost = 1140875.000000000\n",
      "Iter: 370 cost = 1144186.125000000\n",
      "Iter: 380 cost = 1109679.875000000\n",
      "Iter: 390 cost = 1120759.125000000\n",
      "Iter: 400 cost = 1138334.750000000\n",
      "Iter: 410 cost = 1103898.500000000\n",
      "Iter: 420 cost = 1133595.125000000\n",
      "Iter: 430 cost = 1128601.250000000\n",
      "Iter: 440 cost = 1132242.625000000\n",
      "Iter: 450 cost = 1118685.500000000\n",
      "Iter: 460 cost = 1130677.250000000\n",
      "Iter: 470 cost = 1136772.500000000\n",
      "Iter: 480 cost = 1104214.750000000\n",
      "Iter: 490 cost = 1126864.375000000\n",
      "Iter: 500 cost = 1128217.375000000\n",
      "Iter: 510 cost = 1130496.875000000\n",
      "Iter: 520 cost = 1116067.250000000\n",
      "Iter: 530 cost = 1127410.500000000\n",
      "Iter: 540 cost = 1127287.375000000\n",
      "Iter: 550 cost = 1123708.375000000\n",
      "Iter: 560 cost = 1115996.375000000\n",
      "Iter: 570 cost = 1123186.500000000\n",
      "Iter: 580 cost = 1094433.625000000\n",
      "Iter: 590 cost = 1071748.125000000\n",
      "Iter: 600 cost = 1066788.375000000\n",
      "Iter: 610 cost = 1139320.750000000\n",
      "Iter: 620 cost = 1135552.875000000\n",
      "Iter: 630 cost = 1146264.625000000\n",
      "Iter: 640 cost = 1102901.500000000\n",
      "Iter: 650 cost = 1093902.375000000\n",
      "Iter: 660 cost = 1115056.000000000\n",
      "Iter: 670 cost = 1126173.250000000\n",
      "Iter: 680 cost = 1103805.375000000\n",
      "Iter: 690 cost = 1128609.875000000\n",
      "Iter: 700 cost = 1124490.875000000\n",
      "Iter: 710 cost = 1128227.250000000\n",
      "Iter: 720 cost = 1118359.250000000\n",
      "Iter: 730 cost = 1114393.125000000\n",
      "Iter: 740 cost = 1129833.750000000\n",
      "Iter: 750 cost = 1124708.250000000\n",
      "Iter: 760 cost = 1126174.000000000\n",
      "Iter: 770 cost = 1120413.125000000\n",
      "Iter: 780 cost = 1123796.625000000\n",
      "Iter: 790 cost = 1083199.125000000\n",
      "Iter: 800 cost = 1101956.875000000\n",
      "Iter: 810 cost = 1128803.000000000\n",
      "Iter: 820 cost = 1123027.375000000\n",
      "Iter: 830 cost = 1109692.625000000\n",
      "Iter: 840 cost = 1121028.500000000\n",
      "Iter: 850 cost = 1147798.750000000\n",
      "Iter: 860 cost = 1136625.875000000\n",
      "Iter: 870 cost = 1116261.875000000\n",
      "Iter: 880 cost = 1129405.375000000\n",
      "Iter: 890 cost = 1135565.875000000\n",
      "Iter: 900 cost = 1127687.750000000\n",
      "Iter: 910 cost = 1124635.000000000\n",
      "Iter: 920 cost = 1139923.000000000\n",
      "Iter: 930 cost = 1118905.250000000\n",
      "Iter: 940 cost = 1124496.000000000\n",
      "Iter: 950 cost = 1105983.375000000\n",
      "Iter: 960 cost = 1129426.500000000\n",
      "Iter: 970 cost = 1139678.000000000\n",
      "Iter: 980 cost = 1132029.125000000\n",
      "Iter: 990 cost = 1138124.250000000\n",
      "Iter: 1000 cost = 1133100.125000000\n",
      "Iter: 1010 cost = 1127089.625000000\n",
      "Iter: 1020 cost = 1121907.875000000\n",
      "Iter: 1030 cost = 1132095.625000000\n",
      "Iter: 1040 cost = 1120748.625000000\n",
      "Iter: 1050 cost = 1112418.500000000\n",
      "Iter: 1060 cost = 1128359.250000000\n",
      "Iter: 1070 cost = 1097127.125000000\n",
      "Iter: 1080 cost = 1110204.875000000\n",
      "Iter: 1090 cost = 1101605.375000000\n",
      "Iter: 1100 cost = 1140113.000000000\n",
      "Iter: 1110 cost = 1126512.625000000\n",
      "Iter: 1120 cost = 1125643.875000000\n",
      "Iter: 1130 cost = 1130492.750000000\n",
      "Iter: 1140 cost = 1103923.625000000\n",
      "Iter: 1150 cost = 1124773.625000000\n",
      "Iter: 1160 cost = 1115871.500000000\n",
      "Iter: 1170 cost = 1132270.750000000\n",
      "Iter: 1180 cost = 1124514.000000000\n",
      "Iter: 1190 cost = 1131524.750000000\n",
      "Iter: 1200 cost = 1135113.375000000\n",
      "Iter: 1210 cost = 1118953.125000000\n",
      "Iter: 1220 cost = 1130596.375000000\n",
      "Iter: 1230 cost = 1132406.375000000\n",
      "Iter: 1240 cost = 1097535.625000000\n",
      "Iter: 1250 cost = 1111960.750000000\n",
      "Iter: 1260 cost = 1137626.125000000\n",
      "Iter: 1270 cost = 1114773.250000000\n",
      "Iter: 1280 cost = 1131246.750000000\n",
      "Iter: 1290 cost = 1127343.750000000\n",
      "Iter: 1300 cost = 1125811.000000000\n",
      "Iter: 1310 cost = 1140844.375000000\n",
      "Iter: 1320 cost = 1149470.625000000\n",
      "Iter: 1330 cost = 1138011.375000000\n",
      "Iter: 1340 cost = 1135842.250000000\n",
      "Iter: 1350 cost = 1118012.500000000\n",
      "Iter: 1360 cost = 1103010.125000000\n",
      "Iter: 1370 cost = 1133481.625000000\n",
      "Iter: 1380 cost = 1069943.500000000\n",
      "Iter: 1390 cost = 1102306.000000000\n",
      "Iter: 1400 cost = 1136448.375000000\n",
      "Iter: 1410 cost = 1134793.375000000\n",
      "Iter: 1420 cost = 1132422.375000000\n",
      "Iter: 1430 cost = 1092995.125000000\n",
      "Iter: 1440 cost = 1119029.875000000\n",
      "Iter: 1450 cost = 1129161.875000000\n",
      "Iter: 1460 cost = 1137021.000000000\n",
      "Iter: 1470 cost = 1145840.750000000\n",
      "Iter: 1480 cost = 1127873.000000000\n",
      "Iter: 1490 cost = 1137278.500000000\n",
      "Iter: 1500 cost = 1095848.875000000\n",
      "Iter: 1510 cost = 1114314.750000000\n",
      "Iter: 1520 cost = 1115246.500000000\n",
      "Iter: 1530 cost = 1138600.250000000\n",
      "Iter: 1540 cost = 1127806.750000000\n",
      "Iter: 1550 cost = 1141636.500000000\n",
      "Iter: 1560 cost = 1139626.750000000\n",
      "Iter: 1570 cost = 1124662.625000000\n",
      "Iter: 1580 cost = 1139867.000000000\n",
      "Iter: 1590 cost = 1136639.750000000\n",
      "Iter: 1600 cost = 1118703.625000000\n",
      "Iter: 1610 cost = 1107859.125000000\n",
      "Iter: 1620 cost = 1130772.750000000\n",
      "Iter: 1630 cost = 1114436.625000000\n",
      "Iter: 1640 cost = 1125847.250000000\n",
      "Iter: 1650 cost = 1111773.000000000\n",
      "Iter: 1660 cost = 1077187.500000000\n",
      "Iter: 1670 cost = 1140824.625000000\n",
      "Iter: 1680 cost = 1126214.000000000\n",
      "Iter: 1690 cost = 1111493.625000000\n",
      "Iter: 1700 cost = 1115201.875000000\n",
      "Iter: 1710 cost = 1112257.375000000\n",
      "Iter: 1720 cost = 1108137.750000000\n",
      "Iter: 1730 cost = 1132267.000000000\n",
      "Iter: 1740 cost = 1135930.000000000\n",
      "Iter: 1750 cost = 1113670.750000000\n",
      "Iter: 1760 cost = 1129960.500000000\n",
      "Iter: 1770 cost = 1119342.000000000\n",
      "Iter: 1780 cost = 1120858.125000000\n",
      "Iter: 1790 cost = 1110135.750000000\n",
      "Iter: 1800 cost = 1125477.250000000\n",
      "Iter: 1810 cost = 1116763.000000000\n",
      "Iter: 1820 cost = 1130233.750000000\n",
      "Iter: 1830 cost = 1142477.750000000\n",
      "Iter: 1840 cost = 1135213.000000000\n",
      "Iter: 1850 cost = 1117174.000000000\n",
      "Iter: 1860 cost = 1118122.375000000\n",
      "Iter: 1870 cost = 1140016.875000000\n",
      "Iter: 1880 cost = 1116079.750000000\n",
      "Iter: 1890 cost = 1142997.125000000\n",
      "Iter: 1900 cost = 1126070.500000000\n",
      "Iter: 1910 cost = 1122672.250000000\n",
      "Iter: 1920 cost = 1131855.875000000\n",
      "Iter: 1930 cost = 1131730.250000000\n",
      "Iter: 1940 cost = 1119234.875000000\n",
      "Iter: 1950 cost = 1126971.375000000\n",
      "Iter: 1960 cost = 1133766.500000000\n",
      "Iter: 1970 cost = 1118515.250000000\n",
      "Iter: 1980 cost = 1134019.250000000\n",
      "Iter: 1990 cost = 1119860.250000000\n",
      "Iter: 2000 cost = 1088043.750000000\n",
      "Iter: 2010 cost = 1135316.625000000\n",
      "Iter: 2020 cost = 1102054.625000000\n",
      "Iter: 2030 cost = 1118653.875000000\n",
      "Iter: 2040 cost = 1128075.875000000\n",
      "Iter: 2050 cost = 1089495.500000000\n",
      "Iter: 2060 cost = 1128320.000000000\n",
      "Iter: 2070 cost = 1093770.000000000\n",
      "Iter: 2080 cost = 1107395.500000000\n",
      "Iter: 2090 cost = 1123128.250000000\n",
      "Iter: 2100 cost = 1103228.000000000\n",
      "Iter: 2110 cost = 1120591.500000000\n",
      "Iter: 2120 cost = 1136558.375000000\n",
      "Iter: 2130 cost = 1133547.375000000\n",
      "Iter: 2140 cost = 1137797.125000000\n",
      "Iter: 2150 cost = 1139701.625000000\n",
      "Iter: 2160 cost = 1115956.625000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a0790a654224>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# Run optimization op (backprop) and cost op (to get loss value)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "learning_rate = 2\n",
    "training_epochs = 200\n",
    "# 首先计算总批数，保证每次循环训练集中的每个样本都参与训练，不同于批量训练  \n",
    "total_batch = int(raw_data.shape[0]/BATCH_SIZE) #总批数  \n",
    "\n",
    "merged_summary_op = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter('./tmp/logs', sess.graph)\n",
    "total_step = 0\n",
    "\n",
    "for epoch in range(training_epochs):  \n",
    "    index = np.random.permutation(raw_data.shape[0])\n",
    "    for i in range(total_batch):  \n",
    "        total_step += 1\n",
    "        batch_xs= get_batch_np(i, BATCH_SIZE)  # max(x) = 1, min(x) = 0  \n",
    "        \n",
    "        # Run optimization op (backprop) and cost op (to get loss value)  \n",
    "        _, c = sess.run([optimizer, cost], feed_dict={X: batch_xs})\n",
    "        \n",
    "        \n",
    "#         summary_str = sess.run(merged_summary_op, feed_dict={X: batch_xs})\n",
    "#         summary_writer.add_summary(summary_str, total_step)\n",
    "#         summary_writer.add_graph(sess.graph)\n",
    "        \n",
    "        if i % 10 == 0 or i<10:\n",
    "            print \"Iter: %d\" % i, \"cost =\", \"{:.9f}\".format(c) \n",
    "    if epoch % display_step == 0:  \n",
    "        record = open(\"train_loss.txt\", \"a+\")\n",
    "        record.write(\"epoch:\"+str(epoch)+\" loss:\"+\"{:.9f}\\n\".format(c))\n",
    "        record.close()\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c)  \n",
    "        init_op = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "        with tf.Session() as sesstemp:\n",
    "            sesstemp.run(init_op)\n",
    "            saver_path = saver.save(sesstemp, \"model.ckpt\")\n",
    "            print \"Model saved in file: \", saver_path\n",
    "print(\"Optimization Finished!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## classify "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CAPACITY = 30000\n",
    "training_epochs = 10\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[])\n",
    "display_step = 1\n",
    "examples_to_show = 10\n",
    "n_input = 22283 #Need to change to 22283\n",
    "\n",
    "# tf Graph input (only pictures)\n",
    "\n",
    "X = tf.placeholder(\"float\",[None,n_input])\n",
    "y = tf.placeholder(\"float\", shape=[None, 2])  \n",
    "# X = get_batch(data,BATCH_SIZE,CAPACITY)\n",
    "\n",
    "#need to change to 4 layers \n",
    "n_hidden_1 = 200\n",
    "n_hidden_2 = 150\n",
    "n_hidden_3 = 100\n",
    "n_hidden_4 = 42\n",
    "n_class = 2\n",
    "\n",
    "weights= {\n",
    "    'encoder_h1': tf.Variable(tf.truncated_normal([n_input,n_hidden_1], stddev=0.1), name='encoder_h1'),\n",
    "    'encoder_h2': tf.Variable(tf.truncated_normal([n_hidden_1,n_hidden_2], stddev=0.1), name='encoder_h2'),\n",
    "    'encoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_3], stddev=0.1), name='encoder_h3'),\n",
    "    'encoder_h4': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_4], stddev=0.1), name='encoder_h4'),\n",
    "    'decoder_h1': tf.Variable(tf.truncated_normal([n_hidden_4,n_hidden_3], stddev=0.1), name='decoder_h1'),\n",
    "    'decoder_h2': tf.Variable(tf.truncated_normal([n_hidden_3,n_hidden_2], stddev=0.1), name='decoder_h2'),\n",
    "    'decoder_h3': tf.Variable(tf.truncated_normal([n_hidden_2,n_hidden_1], stddev=0.1), name='decoder_h3'),\n",
    "    'decoder_h4': tf.Variable(tf.truncated_normal([n_hidden_1,n_input], stddev=0.1), name='decoder_h4'),\n",
    "}\n",
    "classify = tf.Variable(tf.truncated_normal([n_hidden_4, n_class], stddev=0.1), tf.float32, name='cf')\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1]), name='encoder_b1'),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2]), name='encoder_b2'),\n",
    "    'encoder_b3': tf.Variable(tf.random_normal([n_hidden_3]), name='encoder_b3'),\n",
    "    'encoder_b4': tf.Variable(tf.random_normal([n_hidden_4]), name='encoder_b4'),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_3]), name='decoder_b1'),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_hidden_2]), name='decoder_b2'),\n",
    "    'decoder_b3': tf.Variable(tf.random_normal([n_hidden_1]), name='decoder_b3'),\n",
    "    'decoder_b4': tf.Variable(tf.random_normal([n_input]), name='decoder_b4'),\n",
    "}\n",
    "weightse1_summary = tf.summary.histogram(\"weights_e1\", weights['encoder_h1'])\n",
    "weightse2_summary = tf.summary.histogram(\"weights_e2\", weights['encoder_h2'])\n",
    "weightse3_summary = tf.summary.histogram(\"weights_e3\", weights['encoder_h3'])\n",
    "weightse4_summary = tf.summary.histogram(\"weights_e4\", weights['encoder_h4'])\n",
    "\n",
    "# def encoder(x):  \n",
    "layer_1 = tf.nn.relu(tf.add(tf.matmul(X, weights['encoder_h1']), biases['encoder_b1']))  \n",
    "layer_2 = tf.nn.relu(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))  \n",
    "layer_3 = tf.nn.relu(tf.add(tf.matmul(layer_2, weights['encoder_h3']), biases['encoder_b3']))\n",
    "layer_4 = tf.nn.relu(tf.add(tf.matmul(layer_3, weights['encoder_h4']), biases['encoder_b4']))\n",
    "#     return layer_4\n",
    "encoder_op = layer_4\n",
    "\n",
    "def decoder(x):  \n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['decoder_h4']),  \n",
    "                                   biases['decoder_b4']))  \n",
    "#     layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['decoder_h2']),  \n",
    "#                                    biases['decoder_b2']))  \n",
    "#     layer_3 = tf.nn.tanh(tf.add(tf.matmul(layer_2, weights['decoder_h3']),\n",
    "#                                    biases['decoder_b3']))\n",
    "#     layer_4 = tf.nn.tanh(tf.add(tf.matmul(layer_3, weights['decoder_h4']),\n",
    "#                                    biases['decoder_b4']))\n",
    "    return layer_1\n",
    "\n",
    "def binaryclassify(x):\n",
    "    out = tf.matmul(x, classify)\n",
    "    return out\n",
    "\n",
    "# encoder_op = encoder(X)\n",
    "py = binaryclassify(encoder_op)\n",
    "# cross_entropy = tf.reduce_mean(-tf.reduce_sum(y * tf.log(py), reduction_indices=[1]))\n",
    "# cross_entropy = -tf.reduce_mean(y*tf.log(tf.clip_by_value(py,1e-10,1.0)))\n",
    "cross_entropy = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=py))\n",
    "# decoder_op = decoder(encoder_op)\n",
    "\n",
    "# y_pred = predict\n",
    "# y_true = y\n",
    "\n",
    "# cost = tf.reduce_mean(tf.pow(y_true-y_pred,2))\n",
    "# cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "cost_summary = tf.summary.scalar('cross_entropy', cross_entropy)  \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, name='Adam').minimize(cross_entropy)\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(py,1), tf.argmax(y, 1))\n",
    "trainacc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "trainacc_summary = tf.summary.scalar('trainacc', trainacc) \n",
    "\n",
    "testacc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "testacc_summary = tf.summary.scalar('testacc', testacc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from model.ckpt\n"
     ]
    }
   ],
   "source": [
    "init = tf.global_variables_initializer() \n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "new = 0\n",
    "total_step = 0\n",
    "if new == 1:\n",
    "    sess.run(init)\n",
    "else:\n",
    "    init = tf.global_variables_initializer() \n",
    "    sess.run(init)\n",
    "    saver.restore(sess, 'model.ckpt')\n",
    "    total_step = np.load(\"./total_step.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saver = tf.train.Saver()\n",
    "# sess = tf.Session()\n",
    "# # new_saver = tf.train.import_meta_graph('model.ckpt.meta')\n",
    "# init = tf.global_variables_initializer() \n",
    "# sess.run(init)\n",
    "# saver.restore(sess, 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all_vars = tf.trainable_variables()\n",
    "# for v in all_vars:\n",
    "#     print v.name\n",
    "# sess.run(\"cf:0\")\n",
    "# #     sess.run(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## change test set to one-hot\n",
    "test_index = np.random.permutation(test_data.shape[0])\n",
    "tl_list = test_label[test_index]\n",
    "tl = np.zeros((test_label.shape[0], 2), dtype=np.int)\n",
    "line = 0\n",
    "td = test_data[test_index]\n",
    "for i in tl_list:\n",
    "    tl[line][i] = 1\n",
    "    line += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index = np.random.permutation(train_data.shape[0])\n",
    "def get_batch_np(itert, batchsize):\n",
    "    batch_index = index[itert*batchsize:(itert+1)*batchsize]\n",
    "    batch_data = train_data[batch_index]\n",
    "    label_list = train_label[batch_index]\n",
    "    batch_label = np.zeros((batchsize, 2), dtype=np.int)\n",
    "    line = 0\n",
    "    for i in label_list:\n",
    "        batch_label[line][i] = 1\n",
    "        line += 1\n",
    "    return batch_data, batch_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 0.024553018 trainacc =  0.990\n",
      "TestAcc =  0.931\n",
      "Epoch: 0002 cost= 0.020887913 trainacc =  0.988\n",
      "Epoch: 0003 cost= 0.031545307 trainacc =  0.982\n",
      "Epoch: 0004 cost= 0.021554764 trainacc =  0.990\n",
      "Epoch: 0005 cost= 0.023979973 trainacc =  0.988\n",
      "Epoch: 0006 cost= 0.029179765 trainacc =  0.984\n",
      "Epoch: 0007 cost= 0.018840007 trainacc =  0.988\n",
      "Epoch: 0008 cost= 0.022514639 trainacc =  0.988\n",
      "Epoch: 0009 cost= 0.038621850 trainacc =  0.972\n",
      "Epoch: 0010 cost= 0.024272565 trainacc =  0.988\n",
      "Epoch: 0011 cost= 0.022826971 trainacc =  0.990\n",
      "TestAcc =  0.929\n",
      "Epoch: 0012 cost= 0.026118100 trainacc =  0.986\n",
      "Epoch: 0013 cost= 0.016679393 trainacc =  0.988\n",
      "Epoch: 0014 cost= 0.030096974 trainacc =  0.992\n",
      "Epoch: 0015 cost= 0.028767927 trainacc =  0.978\n",
      "Epoch: 0016 cost= 0.032818168 trainacc =  0.980\n",
      "Epoch: 0017 cost= 0.020022372 trainacc =  0.990\n",
      "Epoch: 0018 cost= 0.033997178 trainacc =  0.980\n",
      "Epoch: 0019 cost= 0.021324193 trainacc =  0.984\n",
      "Epoch: 0020 cost= 0.022154983 trainacc =  0.984\n",
      "Epoch: 0021 cost= 0.033625726 trainacc =  0.980\n",
      "TestAcc =  0.930\n",
      "Epoch: 0022 cost= 0.019760525 trainacc =  0.988\n",
      "Epoch: 0023 cost= 0.021951901 trainacc =  0.992\n",
      "Epoch: 0024 cost= 0.026329026 trainacc =  0.986\n",
      "Epoch: 0025 cost= 0.022487961 trainacc =  0.986\n",
      "Epoch: 0026 cost= 0.030951787 trainacc =  0.984\n",
      "Epoch: 0027 cost= 0.026998375 trainacc =  0.984\n",
      "Epoch: 0028 cost= 0.025846243 trainacc =  0.990\n",
      "Epoch: 0029 cost= 0.026900603 trainacc =  0.980\n",
      "Epoch: 0030 cost= 0.026394183 trainacc =  0.980\n",
      "Epoch: 0031 cost= 0.026687883 trainacc =  0.982\n",
      "TestAcc =  0.930\n",
      "Epoch: 0032 cost= 0.022090208 trainacc =  0.986\n",
      "Epoch: 0033 cost= 0.023299836 trainacc =  0.988\n",
      "Epoch: 0034 cost= 0.025341926 trainacc =  0.982\n",
      "Epoch: 0035 cost= 0.024566367 trainacc =  0.990\n",
      "Epoch: 0036 cost= 0.026357848 trainacc =  0.986\n",
      "Epoch: 0037 cost= 0.016121836 trainacc =  0.994\n",
      "Epoch: 0038 cost= 0.019302659 trainacc =  0.992\n",
      "Epoch: 0039 cost= 0.031803329 trainacc =  0.982\n",
      "Epoch: 0040 cost= 0.019887563 trainacc =  0.988\n",
      "Epoch: 0041 cost= 0.028725868 trainacc =  0.978\n",
      "TestAcc =  0.928\n",
      "Epoch: 0042 cost= 0.029238086 trainacc =  0.980\n",
      "Epoch: 0043 cost= 0.034080818 trainacc =  0.980\n",
      "Epoch: 0044 cost= 0.025712647 trainacc =  0.994\n",
      "Epoch: 0045 cost= 0.021328075 trainacc =  0.992\n",
      "Epoch: 0046 cost= 0.027822796 trainacc =  0.986\n",
      "Epoch: 0047 cost= 0.024597015 trainacc =  0.990\n",
      "Epoch: 0048 cost= 0.021105042 trainacc =  0.992\n",
      "Epoch: 0049 cost= 0.027160706 trainacc =  0.978\n",
      "Epoch: 0050 cost= 0.023990529 trainacc =  0.984\n",
      "Epoch: 0051 cost= 0.020645859 trainacc =  0.990\n",
      "TestAcc =  0.929\n",
      "Epoch: 0052 cost= 0.022263123 trainacc =  0.986\n",
      "Epoch: 0053 cost= 0.020854030 trainacc =  0.988\n",
      "Epoch: 0054 cost= 0.022926921 trainacc =  0.986\n",
      "Epoch: 0055 cost= 0.034139100 trainacc =  0.982\n",
      "Epoch: 0056 cost= 0.012944412 trainacc =  0.994\n",
      "Epoch: 0057 cost= 0.025985938 trainacc =  0.988\n",
      "Epoch: 0058 cost= 0.020979987 trainacc =  0.990\n",
      "Epoch: 0059 cost= 0.041391890 trainacc =  0.972\n",
      "Epoch: 0060 cost= 0.025871364 trainacc =  0.984\n",
      "Epoch: 0061 cost= 0.026045132 trainacc =  0.982\n",
      "TestAcc =  0.928\n",
      "Epoch: 0062 cost= 0.021200620 trainacc =  0.986\n",
      "Epoch: 0063 cost= 0.032152269 trainacc =  0.982\n",
      "Epoch: 0064 cost= 0.021960657 trainacc =  0.992\n",
      "Epoch: 0065 cost= 0.032404497 trainacc =  0.976\n",
      "Epoch: 0066 cost= 0.016097637 trainacc =  0.992\n",
      "Epoch: 0067 cost= 0.020923074 trainacc =  0.990\n",
      "Epoch: 0068 cost= 0.029062008 trainacc =  0.986\n",
      "Epoch: 0069 cost= 0.028962288 trainacc =  0.982\n",
      "Epoch: 0070 cost= 0.026752170 trainacc =  0.986\n",
      "Epoch: 0071 cost= 0.024140030 trainacc =  0.988\n",
      "TestAcc =  0.929\n",
      "Epoch: 0072 cost= 0.023266714 trainacc =  0.990\n",
      "Epoch: 0073 cost= 0.033755362 trainacc =  0.976\n",
      "Epoch: 0074 cost= 0.025329448 trainacc =  0.982\n",
      "Epoch: 0075 cost= 0.034779243 trainacc =  0.974\n",
      "Epoch: 0076 cost= 0.027781410 trainacc =  0.976\n",
      "Epoch: 0077 cost= 0.035650354 trainacc =  0.974\n",
      "Epoch: 0078 cost= 0.034050841 trainacc =  0.978\n",
      "Epoch: 0079 cost= 0.027976276 trainacc =  0.982\n",
      "Epoch: 0080 cost= 0.022467900 trainacc =  0.990\n",
      "Epoch: 0081 cost= 0.022834374 trainacc =  0.990\n",
      "TestAcc =  0.927\n",
      "Epoch: 0082 cost= 0.021044763 trainacc =  0.994\n",
      "Epoch: 0083 cost= 0.032452308 trainacc =  0.986\n",
      "Epoch: 0084 cost= 0.026260041 trainacc =  0.984\n",
      "Epoch: 0085 cost= 0.026351310 trainacc =  0.986\n",
      "Epoch: 0086 cost= 0.029816611 trainacc =  0.982\n",
      "Epoch: 0087 cost= 0.021731500 trainacc =  0.988\n",
      "Epoch: 0088 cost= 0.027027557 trainacc =  0.982\n",
      "Epoch: 0089 cost= 0.027357999 trainacc =  0.988\n",
      "Epoch: 0090 cost= 0.032817092 trainacc =  0.976\n",
      "Epoch: 0091 cost= 0.021355933 trainacc =  0.990\n",
      "TestAcc =  0.929\n",
      "Epoch: 0092 cost= 0.018061863 trainacc =  0.992\n",
      "Epoch: 0093 cost= 0.020569937 trainacc =  0.990\n",
      "Epoch: 0094 cost= 0.023343293 trainacc =  0.992\n",
      "Epoch: 0095 cost= 0.022819847 trainacc =  0.988\n",
      "Epoch: 0096 cost= 0.028884651 trainacc =  0.982\n",
      "Epoch: 0097 cost= 0.030965511 trainacc =  0.976\n",
      "Epoch: 0098 cost= 0.022659430 trainacc =  0.982\n",
      "Epoch: 0099 cost= 0.033865791 trainacc =  0.978\n",
      "Epoch: 0100 cost= 0.034618687 trainacc =  0.978\n",
      "Model saved in file:  model.ckpt\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 500\n",
    "training_epochs = 100\n",
    "test_display_step = 10\n",
    "# 首先计算总批数，保证每次循环训练集中的每个样本都参与训练，不同于批量训练  \n",
    "total_batch = int(train_data.shape[0]/BATCH_SIZE) #总批数  \n",
    " \n",
    "merged_summary_op = tf.summary.merge([cost_summary, weightse1_summary, weightse2_summary, weightse3_summary, weightse4_summary,\n",
    "                                     trainacc_summary])\n",
    "summary_writer = tf.summary.FileWriter('./tmp/logs', sess.graph)\n",
    "summary_writer.add_graph(sess.graph)\n",
    "\n",
    "\n",
    "for epoch in range(training_epochs):  \n",
    "    index = np.random.permutation(train_data.shape[0])\n",
    "    for i in range(total_batch):  \n",
    "        total_step += 1\n",
    "        batch_xs, batch_ys= get_batch_np(i, BATCH_SIZE)  # max(x) = 1, min(x) = 0  \n",
    "        # Run optimization op (backprop) and cost op (to get loss value)\n",
    "        \n",
    "        _, c = sess.run([optimizer, cross_entropy], feed_dict={X: batch_xs, y: batch_ys, learning_rate:0.000001})\n",
    "        \n",
    "        summary_str, _, c, ta = sess.run([merged_summary_op, optimizer, cross_entropy, trainacc], \n",
    "                                               feed_dict={X: batch_xs, y: batch_ys, learning_rate:0.000001})\n",
    "        summary_writer.add_summary(summary_str, total_step)\n",
    "        \n",
    "    if epoch % display_step == 0:  \n",
    "        record = open(\"train_loss.txt\", \"a+\")\n",
    "        record.write(\"epoch:\"+str(epoch)+\" loss:\"+\"{:.9f}\\n\".format(c))\n",
    "        record.close()\n",
    "        print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(c) , \"trainacc = \", \"{:.3f}\".format(ta)\n",
    "    if epoch % test_display_step == 0:\n",
    "        summary_str, tsta = sess.run([tf.summary.merge([testacc_summary]), testacc], feed_dict={X: td, y: tl})\n",
    "        summary_writer.add_summary(summary_str, total_step)\n",
    "        print \"TestAcc = \", \"{:.3f}\".format(tsta)\n",
    "#         correct_prediction = tf.equal(tf.argmax(py,1), tf.cast(y, tf.int64))\n",
    "        \n",
    "#         accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "#         batch_xs, batch_ys= get_batch_np(0, 5000)\n",
    "#         cross_entropy = -tf.reduce_mean(y*tf.log(py))\n",
    "        # cross_entropy2 = -tf.reduce_mean(tf.cast(y, tf.int64)*tf.log(py))\n",
    "#         print(sess.run([accuracy, cross_entropy], feed_dict={X: test_data, y: test_label.reshape(1180,1)}))\n",
    "\n",
    "# sess.run(tf.global_variables_initializer())\n",
    "# saver = tf.train.Saver()\n",
    "saver_path = saver.save(sess, \"model.ckpt\")\n",
    "print \"Model saved in file: \", saver_path\n",
    "np.save(\"total_step\", total_step)\n",
    "print(\"Optimization Finished!\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training set acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85500002]\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(py,1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "batch_xs, batch_ys= get_batch_np(0, 3000)\n",
    "# cross_entropy2 = -tf.reduce_mean(tf.cast(y, tf.int64)*tf.log(py))\n",
    "print(sess.run([accuracy], feed_dict={X: batch_xs, y: batch_ys}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.96866667]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0.96866667]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test set acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.84830511]\n"
     ]
    }
   ],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(py,1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "# batch_xs, batch_ys= get_batch_np(0, 4000)\n",
    "# cross_entropy2 = -tf.reduce_mean(tf.cast(y, tf.int64)*tf.log(py))\n",
    "print(sess.run([accuracy], feed_dict={X: td, y: tl}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[0.92033899]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From <ipython-input-28-707b93bbc41e>:27: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "[[ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]\n",
      " [ 0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1  0.1]]\n",
      "[[  1.74958725e-02   1.74958725e-02   1.74958725e-02   1.74958725e-02\n",
      "    1.74958725e-02   1.74958725e-02   1.74958725e-02   5.23108721e-01\n",
      "    1.74958725e-02   3.36924314e-01]\n",
      " [  1.90278515e-04   1.90278515e-04   1.90278515e-04   1.90278515e-04\n",
      "    1.90278515e-04   1.90278515e-04   1.90278515e-04   1.68166868e-02\n",
      "    1.90278515e-04   9.81661081e-01]]\n",
      "[[  1.77685414e-02   5.68983814e-05   5.68983814e-05   5.68983814e-05\n",
      "    5.68983814e-05   5.68983814e-05   5.68983814e-05   8.84178877e-01\n",
      "    9.21878219e-02   5.52334916e-03]\n",
      " [  6.64212108e-02   6.22349384e-04   6.22349384e-04   6.22349384e-04\n",
      "    6.22349384e-04   6.22349384e-04   6.22349384e-04   1.30644768e-01\n",
      "    7.98876345e-01   3.23597662e-04]]\n",
      "[[  5.18866256e-03   1.24175393e-04   1.24175393e-04   1.24175393e-04\n",
      "    1.24175393e-04   1.24175393e-04   1.24175393e-04   9.90048528e-01\n",
      "    3.84581345e-03   1.71974651e-04]\n",
      " [  8.08557034e-01   1.56743452e-03   1.56743452e-03   1.56743452e-03\n",
      "    1.56743452e-03   1.56743452e-03   1.56743452e-03   6.46150708e-02\n",
      "    1.17416456e-01   6.80501535e-06]]\n",
      "[[  9.65077698e-01   3.28526357e-06   3.48679163e-02   3.28526357e-06\n",
      "    3.28526357e-06   3.28526357e-06   3.28526357e-06   7.98589639e-08\n",
      "    3.78241712e-05   6.56856525e-09]\n",
      " [  4.82075120e-06   6.21139725e-06   9.60010570e-03   6.21139725e-06\n",
      "    6.21139725e-06   6.21139725e-06   6.21139725e-06   3.18950219e-06\n",
      "    2.81674384e-05   9.90332544e-01]]\n",
      "[0.221, array([10000,    10], dtype=int32)]\n"
     ]
    }
   ],
   "source": [
    "#download teh MNIST data in folder \"MNIST_data\" that in the same path as this *.py\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#图片的占位\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "#系数\n",
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "\n",
    "#softmax层\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "\n",
    "#用于训练的真实值占位\n",
    "y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "#交叉熵：-tf.reduce_sum(y_ * tf.log(y)是一个样本的，外面的tf.reduce_mean是batch的\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "\n",
    "#规定训练的方法：注意：使用GradientDescentOptimizer适合上述的误差项\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "#初始化\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "#训练\n",
    "for i in range(5):\n",
    "  batch_xs, batch_ys = mnist.train.next_batch(2)\n",
    "  #print batch_xs.shape\n",
    "  a,_ = sess.run([y, train_step], feed_dict={x: batch_xs, y_: batch_ys})\n",
    "  print a\n",
    "\n",
    "#验证，argmax(y,1)是获得y的第一个维度（即每一行）的最大值的位置\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "print(sess.run([accuracy,tf.shape(y)], feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\",one_hot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
